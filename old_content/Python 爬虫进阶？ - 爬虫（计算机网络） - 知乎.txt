


Python 爬虫进阶？ - 爬虫（计算机网络） - 知乎






--------------------Link http://www.zhihu.com/question/35461941 ----------------------





--------------------Detail----------------------

现在是刚Python入门，也编写了一些简单的爬虫代码，如通过正则，多线程的爬虫，爬取贴吧里面的图片，爬取过代理网站的IP，还接触了scrapy方面的知识。想继续深入下去，还需要做哪些方面的工作，另外还需要看那些方面的书，以及一些开源项目，求各位知乎大神指点下。。。谢谢！！！

-------------------------answer 0 via  -------------------------


我是来吐槽最高票的@Leaf Mohanson虽然学习的确应该追求本质，但是如果一个学习过程太过冗长又没有实质性进展，很容易让人失去继续学习下去的动力。比如说，验证码破解（一般不谈黑产链的活，下不为例），居然推荐了pandas和numpy。如果题主没有相关的基础知识，那么题主需要先学习线性代数、统计和概率、图像识别基础、机器学习基础，然后再来看你推荐的这个K近邻算法，发现原来还需要一堆训练集，好不容易折腾完了之后又发现，卧槽，原来这算法时空复杂度这么高……那么我的推荐是，使用 Google 的 OCR 开源库 tesseract，对应的Python包是pytesser，如果只是做简单（没有数字重叠）的数字识别，那么仅需调用接口就能完成识别。然而关键在于，这压根就应该属于图像识别而不属于爬虫进阶嘛！----------------------------------------------在我看来，不管用什么语言写爬虫，进阶的第一门课一定得是学会自己抓包，分析请求和返回数据。这当中会有一些字段恶心到你，比如通过base64或者md5加密，在模拟登陆验证中通常还会遇到RSA算法。如果你说你懒得学，那么上大杀器Selenium，但是你要忍受它对系统资源的占用（往往要启动浏览器和多个标签页）和不那么快速的爬取速度。针对一些网站的爬取就像是在玩攻防，网站设置了种种反抓取的坑等着你掉进去。这时候你要学会维护好自己的User-Agent，维护好自己的Cookie池，维护好自己的代理IP池，添加恰当的Host和Referer，以让对方服务器觉得这一切看起来都跟真的一模一样，那么你的爬虫开发能力，已经入门了。到此为止，这些知识还和 Python 没有半毛关系，但你知道了要干什么之后，再去搜 Python 相关的工具库，你就会发现原来 Requests 可以轻松构造一个包含自定义 payload 和 headers 的 post 请求；你就会发现原来 Scapy 中可以使用TCP包注入来伪造IP，还能玩SYN FLOOD拒绝服务攻击（误）……所以说，你要做的是爬虫进阶，再用 Python 去寻找一个快捷的实现途径，然后就会发现，还是 Python 大法好，不愧为黑客第一语言。


-------------------------answer 1 via  -------------------------


Python入门网络爬虫之精华版Python学习网络爬虫主要分3个大的版块：抓取，分析，存储另外，比较常用的爬虫框架Scrapy，这里最后也详细介绍一下。首先列举一下本人总结的相关文章，这些覆盖了入门网络爬虫需要的基本概念和技巧：宁哥的小站-网络爬虫当我们在浏览器中输入一个url后回车，后台会发生什么？比如说你输入宁哥的小站（fireling的数据天地）专注网络爬虫、数据挖掘、机器学习方向。，你就会看到宁哥的小站首页。简单来说这段过程发生了以下四个步骤：查找域名对应的IP地址。向IP对应的服务器发送请求。服务器响应请求，发回网页内容。浏览器解析网页内容。网络爬虫要做的，简单来说，就是实现浏览器的功能。通过指定url，直接返回给用户所需要的数据，而不需要一步步人工去操纵浏览器获取。抓取这一步，你要明确要得到的内容是什么？是HTML源码，还是Json格式的字符串等。1. 最基本的抓取抓取大多数情况属于get请求，即直接从对方服务器上获取数据。首先，Python中自带urllib及urllib2这两个模块，基本上能满足一般的页面抓取。另外，requests也是非常有用的包，与此类似的，还有httplib2等等。Requests：
    import requests
    response = requests.get(url)
    content = requests.get(url).content
    print "response headers:", response.headers
    print "content:", content
Urllib2：
    import urllib2
    response = urllib2.urlopen(url)
    content = urllib2.urlopen(url).read()
    print "response headers:", response.headers
    print "content:", content
Httplib2：
    import httplib2
    http = httplib2.Http()
    response_headers, content = http.request(url, 'GET')
    print "response headers:", response_headers
    print "content:", content
此外，对于带有查询字段的url，get请求一般会将来请求的数据附在url之后，以?分割url和传输数据，多个参数用&连接。data = {'data1':'XXXXX', 'data2':'XXXXX'}
Requests：data为dict，json
    import requests
    response = requests.get(url=url, params=data)
Urllib2：data为string
    import urllib, urllib2    
    data = urllib.urlencode(data)
    full_url = url+'?'+data
    response = urllib2.urlopen(full_url)
相关参考：网易新闻排行榜抓取回顾参考项目：网络爬虫之最基本的爬虫：爬取网易新闻排行榜2. 对于登陆情况的处理2.1 使用表单登陆这种情况属于post请求，即先向服务器发送表单数据，服务器再将返回的cookie存入本地。data = {'data1':'XXXXX', 'data2':'XXXXX'}
Requests：data为dict，json
    import requests
    response = requests.post(url=url, data=data)
Urllib2：data为string
    import urllib, urllib2    
    data = urllib.urlencode(data)
    req = urllib2.Request(url=url, data=data)
    response = urllib2.urlopen(req)
2.2 使用cookie登陆使用cookie登陆，服务器会认为你是一个已登陆的用户，所以就会返回给你一个已登陆的内容。因此，需要验证码的情况可以使用带验证码登陆的cookie解决。import requests            
requests_session = requests.session() 
response = requests_session.post(url=url_login, data=data)
若存在验证码，此时采用response = requests_session.post(url=url_login, data=data)是不行的，做法应该如下：response_captcha = requests_session.get(url=url_login, cookies=cookies)
response1 = requests.get(url_login) # 未登陆
response2 = requests_session.get(url_login) # 已登陆，因为之前拿到了Response Cookie！
response3 = requests_session.get(url_results) # 已登陆，因为之前拿到了Response Cookie！
相关参考：网络爬虫-验证码登陆参考项目：网络爬虫之用户名密码及验证码登陆：爬取知乎网站3. 对于反爬虫机制的处理3.1 使用代理适用情况：限制IP地址情况，也可解决由于“频繁点击”而需要输入验证码登陆的情况。这种情况最好的办法就是维护一个代理IP池，网上有很多免费的代理IP，良莠不齐，可以通过筛选找到能用的。对于“频繁点击”的情况，我们还可以通过限制爬虫访问网站的频率来避免被网站禁掉。proxies = {'http':'http://XX.XX.XX.XX:XXXX'}
Requests：
    import requests
    response = requests.get(url=url, proxies=proxies)
Urllib2：
    import urllib2
    proxy_support = urllib2.ProxyHandler(proxies)
    opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)
    urllib2.install_opener(opener) # 安装opener，此后调用urlopen()时都会使用安装过的opener对象
    response = urllib2.urlopen(url)
3.2 时间设置适用情况：限制频率情况。Requests，Urllib2都可以使用time库的sleep()函数：import time
time.sleep(1)
3.3 伪装成浏览器，或者反“反盗链”有些网站会检查你是不是真的浏览器访问，还是机器自动访问的。这种情况，加上User-Agent，表明你是浏览器访问即可。有时还会检查是否带Referer信息还会检查你的Referer是否合法，一般再加上Referer。headers = {'User-Agent':'XXXXX'} # 伪装成浏览器访问，适用于拒绝爬虫的网站
headers = {'Referer':'XXXXX'}
headers = {'User-Agent':'XXXXX', 'Referer':'XXXXX'}
Requests：
    response = requests.get(url=url, headers=headers)
Urllib2：
    import urllib, urllib2   
    req = urllib2.Request(url=url, headers=headers)
    response = urllib2.urlopen(req)
4. 对于断线重连不多说。def multi_session(session, *arg):
    retryTimes = 20
    while retryTimes>0:
        try:
            return session.post(*arg)
        except:
            print '.',
            retryTimes -= 1
或者def multi_open(opener, *arg):
    retryTimes = 20
    while retryTimes>0:
        try:
            return opener.open(*arg)
        except:
            print '.',
            retryTimes -= 1
这样我们就可以使用multi_session或multi_open对爬虫抓取的session或opener进行保持。5. 多进程抓取这里针对华尔街见闻进行并行抓取的实验对比：Python多进程抓取 与 Java单线程和多线程抓取相关参考：关于Python和Java的多进程多线程计算方法对比6. 对于Ajax请求的处理对于“加载更多”情况，使用Ajax来传输很多数据。它的工作原理是：从网页的url加载网页的源代码之后，会在浏览器里执行JavaScript程序。这些程序会加载更多的内容，“填充”到网页里。这就是为什么如果你直接去爬网页本身的url，你会找不到页面的实际内容。这里，若使用Google Chrome分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。如果“请求”之前有页面，依据上一步的网址进行分析推导第1页。以此类推，抓取抓Ajax地址的数据。对返回的json格式数据(str)进行正则匹配。json格式数据中，需从'\uxxxx'形式的unicode_escape编码转换成u'\uxxxx'的unicode编码。7. 自动化测试工具SeleniumSelenium是一款自动化测试工具。它能实现操纵浏览器，包括字符填充、鼠标点击、获取元素、页面切换等一系列操作。总之，凡是浏览器能做的事，Selenium都能够做到。这里列出在给定城市列表后，使用selenium来动态抓取去哪儿网的票价信息的代码。参考项目：网络爬虫之Selenium使用代理登陆：爬取去哪儿网站8. 验证码识别对于网站有验证码的情况，我们有三种办法：使用代理，更新IP。使用cookie登陆。验证码识别。使用代理和使用cookie登陆之前已经讲过，下面讲一下验证码识别。可以利用开源的Tesseract-OCR系统进行验证码图片的下载及识别，将识别的字符传到爬虫系统进行模拟登陆。当然也可以将验证码图片上传到打码平台上进行识别。如果不成功，可以再次更新验证码识别，直到成功为止。参考项目：Captcha1爬取有两个需要注意的问题：如何监控一系列网站的更新情况，也就是说，如何进行增量式爬取？对于海量数据，如何实现分布式爬取？分析抓取之后就是对抓取的内容进行分析，你需要什么内容，就从中提炼出相关的内容来。常见的分析工具有正则表达式，BeautifulSoup，lxml等等。存储分析出我们需要的内容之后，接下来就是存储了。我们可以选择存入文本文件，也可以选择存入MySQL或MongoDB数据库等。存储有两个需要注意的问题：如何进行网页去重？内容以什么形式存储？ScrapyScrapy是一个基于Twisted的开源的Python爬虫框架，在工业中应用非常广泛。相关内容可以参考基于Scrapy网络爬虫的搭建，同时给出这篇文章介绍的微信搜索爬取的项目代码，给大家作为学习参考。参考项目：使用Scrapy或Requests递归抓取微信搜索结果


-------------------------answer 2 via  -------------------------


根据题主的描述，我猜测应该是已经根据网上的一些教程、博客写爬虫抓取过一些简单的内容，然后想要继续深入的时候，发现网上更进一步的学习资源不那么好找了。会抓取贴吧图片（猜测是网上教程抓取一个帖子下的图片而不是全贴吧）、能够使用多线程、抓取代理 IP、有 scrapy 的经验，接下来该怎么做，给你一点建议（这个问题是大半年以前提的，估计你已经不需要我的建议了 ^_^），仅供参考，错了你也没法把我怎么样~~~首先，可以多抓取一些代理 IP，并不断更新，以及有一套完整的校验代理 IP 可用性和淘汰过期代理的程序，你的代理库要随时拥有五位数的可用代理 IP，并保证不可用 IP 会在失效后较短时间被剔除出去。大部分情况下，爬虫的效率瓶颈并不在你开几个线程，多大并行。因为大部分商业网站（个人博客、小网站和一些反爬虫较弱的站除外）都会根据你的访问频率限制，所以太快之后分分钟被封，你需要很多代理 IP， 在被封之后可以迅速切换新的 IP 继续抓取。其次，你可以锻炼自己抓取复杂网页的能力。你可以尝试做网站登录，到前端渲染异常丰富的网站的抓取都是练手的好机会。切记，对于稍微复杂的验证码，不要去做所谓破解验证码，这需要较强的基础知识（包括但不限于统计学、图像识别、机器学习、...），可能还没有识别出验证码，你就先失去了兴趣。先手工打码，做你正在做的事情——爬虫。先去尝试登录豆瓣、人人，然后去尝试微博，再到 Google 这样两步验证的网站。前端复杂的动态网站，去尝试微博、QQ空间等等吧。然后，考虑那种会随时增加内容的网站，如何增量式抓取数据。比如，58 每天会产生新的招聘信息，如何只是增量式抓取这部分新增数据，而不需要重复抓取已有数据。这需要考虑如何设计存储。增量式的抓取可以帮你实现在最小化资源的情况下对一个网站进行数据监控。然后，尝试抓取大量的数据。大量值得是那种单机基本上搞不定的网站，就算可以搞定，也一定要多搞几台机器弄成分布式抓取。爬虫的分布式不同于你想象中的分布式，你仅仅需要控制一个任务生成端、一个任务分发端和一批爬虫消费任务即可。最后，尝试融合上面的内容，就基本可以做到「只要浏览器可以打开，我就可以抓取到」的水平了。---------------------爬虫无非三步：下载源码抽取数据存储数据所以，你需要考虑的是：如何高效的抓取如何抽取有用的数据如何设计存储结构如何近乎实时的更新如何判重并减少冗余数据存储---------------------书籍方面，不负责任地推荐一本：Python网络数据采集 (豆瓣) 。


-------------------------answer 3 via  -------------------------


务虚地说说爬虫进阶。爬虫写得多了，就感到有些乏。这个乏，指的并不是乏味，而是更广一些的，浑身使不上劲的那种乏。从务实的角度看，现有的答案已经回答地非常全面，无可指摘了。相信大多数人的爬虫入门都和我类似，先从urllib2 入手，写一个最简陋的get，面对一大堆源码无所适从。接着开始接触传说中给人用的requests，惊呼『这简直是太棒了』。在requests 的学习中，我们知道了proxy，知道了user-agent，知道了如何post。随后，我们开始放下写的头疼的正则表达式（regex），开始了解xpath，BeautifulSoup，又是一阵惊呼。我们攻克了知乎（曾经），攻克了移动端的微博，却卡在了网页版的微博。于是我们知道了selenium, 用上了PhantomJS，好嘛，现在浏览器能做的我都能做了。渐渐地我们不满足于单线程的慢慢虫，于是我们开始写多线程。渐渐地我们不满足于把数据放在csv文件中，于是我们开始用上mysql，mongodb，redis。最终我们发现从头开始写一个爬虫太不划算了，于是我们又捡起了一开始曾接触但随即放弃了的scrapy。乖乖，现在我们才发现scrapy的速度那么快，效率那么高。我们不禁有些泄气。如果你看到这里，能够感同身受的话，我们的水平也就差不太多了，如果你觉得上述我所说的只是些小儿科的话，那我的水平有限，接下来的内容你可以随意浏览，权当娱乐。我想，开头提到的乏，其实是对自己所做事情的意义不明晰导致的。我们明白，先有数据，后有爬虫。所有的爬虫都只能收集数据，分析数据，而不能直接产生数据。是的，即使是那些分析的结果，也必须建立在原始的数据之上。这里我们不讨论涉及黑产的爬虫（即搜集到的数据本身就可以卖钱）。除去这一类，爬虫最有价值的一环，正是数据的分析结果。因为爬虫能拿到的所有数据都是公开的，免费的，所以唯有将这些数据清洗，重塑，并分析之后，你才能得到全新的，属于你自己的信息。可是，要如何分析收集到的数据呢？你会发现，光是凭借计算机的知识，是无法做到深入分析这一点的。你能做的，无非是求求和，求求均值方差，画画统计图表，描描趋势图。分析结果的呈现方式有很多，有些显得很low，有些显得很高大上。但本质上它们没有太大区别，它们的价值是很有限的。更好一些的分析手段还有机器学习，也许你可以做一个推荐系统，或是做一个聚类分析。可是从根本上来说，想要最大程度地利用通过爬虫获得的数据，需要的首先是强大的数学基础，其次是数据来源相关学科的学科背景，比如搜集的是经济数据，那么就要求你有很强的经济学功底和很好的市场嗅觉，而这已经完全脱离了爬虫的范围。对于大多数像我一样的票友来讲，我们写爬虫追求的并不是效率，也不是规模，甚至不是数据本身。我们追求的，是万千数据中提炼出有价值的那一部分，并把那一部分为自己所用。因此，如果你的志向不是成为一名爬虫工程师的话，我对于爬虫进阶的建议，从务虚的角度讲，应该是修炼好自己本学科的内功。爬虫是手段而非目的，或许明白了这一点，你的爬虫就能更进一步了。


-------------------------answer 4 via  -------------------------


爬虫不就是个cosplay嘛。高票回答提到的User-Agent什么的，就凭这些就想cosplay浏览器？你也太小瞧浏览器了。某大型票务网站，第一步请求会得到一个含Json的html，第二步请求把这个Json POST上去，不过如果你兴致勃勃把html里的Json抠出来再POST上去，发现屁都得不到。header早就加满了，Cookie也由CookieJar维护着，问题出在哪里？问题出在第一步请求里得到的html里有一段JavaScript，这段JavaScript大概长这样：eval(function(p, a, c, k, e, d) {
            e = function(c) {
                return (c < a ? "" : e(parseInt(c / a))) + ((c = c % a) > 35 ? String.fromCharCode(c + 29) : c.toString(36))
            }
            ;
            if (!''.replace(/^/, String)) {
                while (c--)
                    d[e(c)] = k[c] || e(c);
                k = [function(e) {
                    return d[e]
                }
                ];
                e = function() {
                    return '\\w+'
                }
                ;
                c = 1;
            }
            ;while (c--)
                if (k[c])
                    p = p.replace(new RegExp('\\b' + e(c) + '\\b','g'), k[c]);
            return p;
        }('m 5$=[\'\',\'b\',\'f\',\'e\',\'h\'],l,7;g(6[5$[1]]){l=7.9(5$[0]);c=l.8(d,a);l.8(i,j,c);6[5$[4]]=6[5$[1]](6[5$[2]]=6[5$[2]][5$[3]](7,l.k(5$[0])))}', 23, 23, '|||||_|w|r|splice|split|0x1|simpleLoader||y|replace|condition|if|flightLoader|x|0x0|join||var'.split('|'), 0, {}))
看不懂就对了，人家就是为了让你看不懂，以上还只是其中一小部分。总而言之，这段恶心的JS的作用就是用来修改html里的那段Json，至于怎么修改的嘛，单步调试去吧，很开心的~或者是另一个大型票务网站，在window对象下生成了一个属性叫做UA_obj，生成这个属性的代码长成这样：function(_rsk) {
    var _yqrm, _nre, _vrd, _myf, _tbv, _yszz, _gqqw, _myqi = function() {
        var _tmza = "rAt"
          , _xba = "rAt"
          , _roeu = "gth"
          , _euq = "len"
          , _cixk = "str"
          , _wyfa = "sub"
          , _mng = "str"
          , _zaat = "str"
          , _lzvw = "str"
          , _zzp = "sub"
          , _orb = "gth"
          , _tbv = "in"
          , _opxz = "erse"
          , _ywr = "rev"
          , _oru = "lit"
          , _sma = "sp"
          , _shbx = "gth"
          , _govz = -((104 | 3525868) % 705192)
          , _masd = (0 | 1411056) % 705954
          , _hurz = (144 | 367313) % 122771
          , _iewg = -((271392 | 1342758) % 447635)
          , _ejp = (2576 | 4172372) % 834567
          , _hce = -((549896 | 1668236) % 834132)
          , _qzj = (8 | 2016360) % 672356
          , _oje = -((0 | 4030256) % 671722)
          , _sii = -((24 | 400446) % 80163)
          , _floi = (1146888 | 1558344) % 519753
          , _ckz = (4228481 | 4302721) % 717263
          , _ppe = -((1351712 | 3585073) % 717167)
          , _lbvz = (522 | 4900442) % 980260
          , _qebr = -((83 | 1958867) % 979465)
          , _mayb = (4144 | 2864689) % 716268
          , _qyad = (0 | 2432780) % 608389
          , _slc = Object();
        return function(_nqb, _ybqc, _rsk) {
            var _myqi, _iacy, _gjv, _jvy, _brn, _zer, _wuq, _fhv = "str", _aaen = "sub", _qvj = "cha", _tvf = "str", _gqb = "sub", _rhmz = "cha", _atsx = "sub", _ggo = "rAt", _zcw = "cha", _ttt = "sub", _ixho = "rAt", _wbcx = "cha", _kqgl = "len", _myf = "jo", _heec = "len", _owqm = "str", _som = "sub", _gjro = (2236856 | 3876287) % 775435, _huz = -((131337 | 3101017) % 775490), _bmc = -((268297 | 2472239) % 494591), _nng = (3 | 2470935) % 494265, _lvk = -((3928 | 733150) % 122276), _kfrq = -((1582082 | 3714198) % 742916), _xjfs = (2097459 | 2228663) % 743064, _kemr = -((2097217 | 2642385) % 440486), _mdq = (4264 | 2202792) % 440709, _zqdo = (56576 | 3923944) % 784818, _ryt = -((1126977 | 1569379) % 784707), _epw = (1326 | 2688303) % 448163, _csu = (4260002 | 5310374) % 885148, _qoax = -((72 | 1769818) % 885184), _bas = (167 | 401575) % 80445, _ittx = -((10 | 3113802) % 518993), _eoqi = -((1282 | 501042) % 167078), _ehv = (8 | 334604) % 167717, _xfot = -((0 | 2865233) % 716450), _zat = -((4 | 2431573) % 607987);
            if (_slc[_nqb])
                return _slc[_nqb];
            _myqi = _ybqc + _rsk;
            _iacy = "",
            _gjv = "";
            _jvy = _nqb;
            _nqb = _nqb[_som + _owqm](_zat + _qyad, _nqb[_heec + _shbx] - (_xfot + _mayb));
            _nqb = _nqb[_sma + _oru]("")[_ywr + _opxz]()[_myf + _tbv]("");
            _brn = _nqb[_kqgl + _orb];
            for (_zer = _qebr + _lbvz; _zer < _brn; ++_zer)
                if (_zer % _myqi == _myqi - (_ehv + _eoqi)) {
                    _gjv = _nqb[_zzp + _lzvw](_brn - _zer - (_ppe + _ckz), _myqi);
                    _gjv = _gjv[_wbcx + _ixho](_myqi - (_ittx + _floi)) + _gjv[_ttt + _zaat](_bas + _sii, _myqi - (_oje + _qzj)) + _gjv[_zcw + _ggo](_hce + _ejp);
                    _gjv = _gjv[_atsx + _mng](_qoax + _csu, _ybqc);
                    _iacy += _gjv
                }
            _wuq = _brn % _myqi;
            if (_wuq != _epw + _iewg) {
                _gjv = _nqb[_wyfa + _cixk](_ryt + _zqdo, _wuq);
                if (_gjv[_euq + _roeu] != _mdq + _kemr)
                    _gjv = _gjv[_rhmz + _xba](_wuq - (_xjfs + _kfrq)) + _gjv[_gqb + _tvf](_lvk + _hurz, _wuq - (_masd + _govz)) + _gjv[_qvj + _tmza](_nng + _bmc);
                _gjv = _gjv[_aaen + _fhv](_huz + _gjro, _ybqc);
                _iacy += _gjv
            }
            _slc[_jvy] = _iacy;
            return _iacy
        }
好啦我只是随便截取一段，括号并不闭合。里面迷一样的条件分支让你断点都不知道怎么打，单步运行半个小时都不知道UA_obj属性是什么时候加上去的。所以重点是什么？爬虫固然会涉及到Proxy池、Cookie池、头，但是在真正的魔鬼级别的爬虫却是要求你在JS里长袖善舞。===============================至于其他的一切技术：多线程、集群等等，都是建立在单线程爬虫没问题的基础上。不过不知道为什么大家都特别喜欢多线程爬虫，明明多路复用比多线程效率快几个数量级来着。Tornado的IOLoop就可以直接拿来用，不过如果只是简单的爬虫需求，不如自己写个事件循环就好了，窗子用户就算没有epoll用select也很好啊。或者一言不合重新造个多路复用的包，在httplib上包装一个连接池、Cookie池、自动重定向功能，我看比那些只会调用scrapy接口的水笔高到不知道哪里去了。那么只有到了这个时候，我们才能说，我会爬虫了。因为衡量会不会爬虫的标准只有一点：如果浏览器能够得到，那么我的爬虫也一定能够得到。


-------------------------answer 5 via  -------------------------


一、gzip/deflate支持现在的网页普遍支持gzip压缩，这往往可以解决大量传输时间，以VeryCD的主页为例，未压缩版本247K，压缩了以后45K，为原来的1/5。这就意味着抓取速度会快5倍。然而python的urllib/urllib2默认都不支持压缩，要返回压缩格式，必须在request的header里面写明’accept-encoding’，然后读取response后更要检查header查看是否有’content-encoding’一项来判断是否需要解码，很繁琐琐碎。如何让urllib2自动支持gzip, defalte呢？其实可以继承BaseHanlder类，然后build_opener的方式来处理：import urllib2 from gzip import GzipFile from StringIO import StringIO class ContentEncodingProcessor(urllib2.BaseHandler):   """A handler to add gzip capabilities to urllib2 requests """     # add headers to requests   def http_request(self, req):     req.add_header("Accept-Encoding", "gzip, deflate")     return req     # decode   def http_response(self, req, resp):     old_resp = resp     # gzip     if resp.headers.get("content-encoding") == "gzip":         gz = GzipFile(                     fileobj=StringIO(resp.read()),                     mode="r"                   )         resp = urllib2.addinfourl(gz, old_resp.headers, old_resp.url, old_resp.code)         resp.msg = old_resp.msg     # deflate     if resp.headers.get("content-encoding") == "deflate":         gz = StringIO( deflate(resp.read()) )         resp = urllib2.addinfourl(gz, old_resp.headers, old_resp.url, old_resp.code)  # 'class to add info() and         resp.msg = old_resp.msg     return resp   # deflate support import zlib def deflate(data):   # zlib only provides the zlib compress format, not the deflate format;   try:               # so on top of all there's this workaround:     return zlib.decompress(data, -zlib.MAX_WBITS)   except zlib.error:     return zlib.decompress(data)
然后就简单了，encoding_support = ContentEncodingProcessor opener = urllib2.build_opener( encoding_support, urllib2.HTTPHandler )   #直接用opener打开网页，如果服务器支持gzip/defalte则自动解压缩 content = opener.open(url).read()
二、更方便地多线程总结一文的确提及了一个简单的多线程模板，但是那个东东真正应用到程序里面去只会让程序变得支离破碎，不堪入目。在怎么更方便地进行多线程方面我也动了一番脑筋。先想想怎么进行多线程调用最方便呢？1、用twisted进行异步I/O抓取事实上更高效的抓取并非一定要用多线程，也可以使用异步I/O法：直接用twisted的getPage方法，然后分别加上异步I/O结束时的callback和errback方法即可。例如可以这么干：from twisted.web.client import getPage from twisted.internet import reactor   links = [ 'http://www.verycd.com/topics/%d/'%i for i in range(5420,5430) ]   def parse_page(data,url):     print len(data),url   def fetch_error(error,url):     print error.getErrorMessage(),url   # 批量抓取链接 for url in links:     getPage(url,timeout=5) \         .addCallback(parse_page,url) \ #成功则调用parse_page方法         .addErrback(fetch_error,url)     #失败则调用fetch_error方法   reactor.callLater(5, reactor.stop) #5秒钟后通知reactor结束程序 reactor.run()
twisted人如其名，写的代码实在是太扭曲了，非正常人所能接受，虽然这个简单的例子看上去还好；每次写twisted的程序整个人都扭曲了，累得不得了，文档等于没有，必须得看源码才知道怎么整，唉不提了。如果要支持gzip/deflate，甚至做一些登陆的扩展，就得为twisted写个新的HTTPClientFactory类诸如此类，我这眉头真是大皱，遂放弃。有毅力者请自行尝试。这篇讲怎么用twisted来进行批量网址处理的文章不错，由浅入深，深入浅出，可以一看。2、设计一个简单的多线程抓取类还是觉得在urllib之类python“本土”的东东里面折腾起来更舒服。试想一下，如果有个Fetcher类，你可以这么调用f = Fetcher(threads=10) #设定下载线程数为10 for url in urls:     f.push(url)  #把所有url推入下载队列 while f.taskleft(): #若还有未完成下载的线程     content = f.pop()  #从下载完成队列中取出结果     do_with(content) # 处理content内容
这么个多线程调用简单明了，那么就这么设计吧，首先要有两个队列，用Queue搞定，多线程的基本架构也和“技巧总结”一文类似，push方法和pop方法都比较好处理，都是直接用Queue的方法，taskleft则是如果有“正在运行的任务”或者”队列中的任务”则为是，也好办，于是代码如下：import urllib2 from threading import Thread,Lock from Queue import Queue import time   class Fetcher:     def __init__(self,threads):         self.opener = urllib2.build_opener(urllib2.HTTPHandler)         self.lock = Lock() #线程锁         self.q_req = Queue() #任务队列         self.q_ans = Queue() #完成队列         self.threads = threads         for i in range(threads):             t = Thread(target=self.threadget)             t.setDaemon(True)             t.start()         self.running = 0       def __del__(self): #解构时需等待两个队列完成         time.sleep(0.5)         self.q_req.join()         self.q_ans.join()       def taskleft(self):         return self.q_req.qsize()+self.q_ans.qsize()+self.running       def push(self,req):         self.q_req.put(req)       def pop(self):         return self.q_ans.get()       def threadget(self):         while True:             req = self.q_req.get()             with self.lock: #要保证该操作的原子性，进入critical area                 self.running += 1             try:                 ans = self.opener.open(req).read()             except Exception, what:                 ans = ''                 print what             self.q_ans.put((req,ans))             with self.lock:                 self.running -= 1             self.q_req.task_done()             time.sleep(0.1) # don't spam   if __name__ == "__main__":     links = [ 'http://www.verycd.com/topics/%d/'%i for i in range(5420,5430) ]     f = Fetcher(threads=10)     for url in links:         f.push(url)     while f.taskleft():         url,content = f.pop()         print url,len(content)

以上为其他先入之士的内容，欢迎一起学习和交流。



-------------------------answer 6 via  -------------------------


做过一段时间爬虫 freelancer, 接过5个项目.0. requests 模块, beautifulsoup模块, css选择器语法, re 正则模块, http 头编写, cookies, json解析等一定要掌握至熟练及以上程度.1. 爬取重 ajax 页面, 推荐谷歌优先搜索 phantomjs, 其次selenium. 2. 破解图片验证码, 推荐谷歌开源库 pytesser (感谢 @simons 的吐槽), 进一步深入可以学习<高等数学-线性代数>, 谷歌搜索 pandas, numpy, k近邻算法.3. 过滤器, 推荐谷歌搜索 布隆过滤器 及其c实现; 谷歌搜索 python 如何导入c模块.4. 分布式爬虫(消息队列). 推荐谷歌搜索 rabbitmq.5. 任务调度. 推荐谷歌搜索 schedule.基本上是这样一个学习阶梯. 另: 反对学习任何爬虫框架, 尤其 scrapy, pyspider. 原因：这两个框架太优秀，太全。年迈的程序员可以去看源码，年轻的程序员还是自己多动手。-----------------------更新 by Mohanson, 20150919----------------------1: 关于是否应该学习爬虫框架: 我个人学过 scrapy, pyspider, 前期用的比较多, 后期基本纯手打，用的技术都在上面介绍过了。如果是学web开发，我一定会推荐tornado框架，如果谁跟我说别学框架，我一定打死他。2. 爬的时候注意素质, 每秒10个请求就差不多了.我见过有人搞500路并发把人家服务搞挂掉的.不要给别人添麻烦, 也不要给自己惹上麻烦.**爬虫不是性能测试**, **爬虫不是性能测试**, **爬虫不是性能测试**.-----------------------更新 by Mohanson, 20160311----------------------时隔半年依然陆续有赞收到, 谢谢.目前已经不做爬虫了, 但最近在做的一些工作可能对大家会有点帮助.如我上面所说,"每秒10个请求就差不多了"这个要求看似非常简单, 但我相信会有少部分程序员在单机情况下无法实现. 下面来介绍下如何在PC上实现这个要求.简单测试一下(阿里云服务器, 1M带宽)time curl http://www.baidu.com耗时 0.454 秒.换句话说, 在不考虑数据处理, 数据存储的情况下, 每秒只够请求百度两次.OK, 为什么?因为等待与IO占用了大部分时间.解决这种占着CPU不拉屎的情况, 可行的方法是线程池与异步IO或异步回调.我最近切换到了 python3.5, 异步网络 io 已经有官方库帮着做的.请戳下面, 自己学习.18.5. asyncio â€“ Asynchronous I/O, event loop, coroutines and tasks线程池更简单, 可以自己写也可以用轮子.class WPooler:

    def __init__(self, maxsize=1024, concurrent=4):
        self.queue = queue.Queue(maxsize=maxsize)
        self.concurrent = concurrent
        self.threads = []

        for i in range(self.concurrent):
            self.threads.append(WThreader(self.queue))

        for thread in self.threads:
            thread.start()

    def do(self, func, args=(), kwargs={}, callback=None):
        self.queue.put((func, args, kwargs, callback))

    def async(self, callback=None):
        return Asyncer(self, callback=callback)

    def wait(self):
        self.queue.join()


class WThreader(threading.Thread):

    def __init__(self, queue):
        threading.Thread.__init__(self, daemon=True)
        self.queue = queue

    def run(self):
        while True:
            func, args, kwargs, callback = self.queue.get(block=True)
            try:
                r = func(*args, **kwargs)
                if callback:
                    callback(r)
            except Exception as e:
                logging.exception(e)
            finally:
                self.queue.task_done()



-------------------------answer 7 via  -------------------------


爬虫是在没有（用）API获取数据的情况下以Hack的方式获取数据的一种有效手段；进阶，就是从爬取简单页面逐渐过渡到复杂页面的过程。针对特定需求，爬取的网站类型不同，可以使用不同的python库相结合，达到快速抓取数据的目的。但是无论使用什么库，第一步分析目标网页的页面元素发现抓取规律总是必不可少的：有些爬虫是通过访问固定url前缀拼接不同的后缀进行循环抓取，有些是通过一个起始url作为种子url继而获取更多的目标url递归抓取；有些网页是静态数据可以直接获取，有些网页是js渲染数据需要构造二次请求……如果统统都写下来，一篇文章是不够的，这里举几个典型的栗子：1. 页面url为固定url前缀拼接不同的后缀：以从OPENISBN网站抓取图书分类信息为例，我有一批图书需要入库，但是图书信息不全，比如缺少图书分类，此时需要去"http://openisbn.com/"网站根据ISBN号获取图书的分类信息。如《失控》这本书， ISBN: 7513300712 ，对应url为 "http://openisbn.com/isbn/7513300712/ " ，分析url规律就是以 "http://openisbn.com/isbn/" 作为固定前缀然后拼接ISBN号得到；然后分析页面元素，Chrome右键 —> 检查：我先直接使用urllib2 + re 来获得“Category:” 信息:#-*- coding:UTF-8 -*-

import re
import urllib2


isbn = '7513300712'
url = 'http://openisbn.com/isbn/{0}/'.format(isbn)
category_pattern = re.compile(r'Category: *.*, ')
html = urllib2.urlopen(url).read()
category_info = category_pattern.findall(html)

if len(category_info) > 0 :
    print category_info[0]
else:
    print 'get category failed.'
输出：Category: 现当代小说, 小说, 2.选择合适的定位元素：由于页面中只有一行“Category:” 信息，正则表达式提取就行，如果有很多行的话就需要缩小查找范围了，BeautifulSoup库就可以用来定位查找范围。通过分析可知，包含所需“Category:” 最近一层的div 是 <div class=“PostContent”>，仔细观察，外层还有一个 <div class=“PostContent”>，而 <div class=“Post”> 也是一样，这样如果使用它们来定位范围的话，使用find方法返回的tag对象是最先找到的外层div，范围不够小；使用findAll，返回的tag对象列表还需要遍历，综合得出用<div class=“Article”> 作为定位元素，find方法定位返回的范围够小，又不需要对find结果进行遍历。使用urllib2 + Beautiful Soup 3  + re 再来提取一次 （Beautiful Soup最新版本为4.4，兼容python3和python2，BS4跟BS3在导包方式上有点差别）：#-*- coding:UTF-8 -*-

import re
import urllib2
from BeautifulSoup import BeautifulSoup


isbn = '7513300712'
url = 'http://openisbn.com/isbn/{0}/'.format(isbn)
category_pattern = re.compile(r'Category: *.*, ')
html = urllib2.urlopen(url).read()
soup = BeautifulSoup(html)
div_tag = soup.find('div',{'class':'Article'})
category_info = category_pattern.findall(str(div_tag))

if len(category_info) > 0 :
    print category_info[0]
else:
    print 'get category failed.'
输出：Category: 现当代小说, 小说, 3. 抓取js渲染的内容：用baidu搜索日历，获取结果页中的节假日日期像上次一样直接使用urllib打开网页，发现返回的html中并没有期望得到的内容，原因是我通过浏览器所看到的页面内容实际是等js渲染完成后最终展现的，中间还包含了多次的ajax请求，这样使用urllib一次就不能胜任了，此时就可以让selenium上场了(webdriver用的phantomjs，需要提前下载phantomjs放到当前的PATH路径下)，由于要查找的标识 <div class=“op-calendar-new-relative”> 包含了多个，所以这次使用的方法是findAll，然后再对返回的结果进行遍历，解析每个tag对象的a属性，如果包含了“休”字标识，那么这一天就是节假日。# -*- coding:UTF-8 -*-

import re
import urllib
from selenium import webdriver
from BeautifulSoup import BeautifulSoup


holiday_list = []
url = 'http://www.baidu.com/s?' + urllib.urlencode({'wd': '日历'})
date_pattern = re.compile(r'date="[\d]+[-][\d]+[-][\d]+"')

driver = webdriver.PhantomJS()
driver.get(url)
html = driver.page_source
driver.quit()

soup = BeautifulSoup(html)
td_div_list = soup.findAll('div',{'class':'op-calendar-new-relative'})
for td_tag in td_div_list:
    href_tag = str(td_tag.a)
    if href_tag.find('休') != -1:
        holiday_date_list = date_pattern.findall(href_tag)
        if len(holiday_date_list) > 0:
            holiday_list.append(holiday_date_list[0].split('"')[1])

print holiday_list
输出：['2016-4-2', '2016-4-3', '2016-4-4', '2016-4-30', '2016-5-1’]4. 设置代理，抓取google play排行榜（selenium不仅可以很好的模拟浏览器行为，还可以将网页内容截图保存）# -*- coding:UTF-8 -*-

from selenium import webdriver

url = 'https://play.google.com/store/apps/top?hl=zh_CN'
proxy_setting = ['--proxy=127.0.0.1:10800', '--proxy-type=socks5']
driver = webdriver.PhantomJS(service_args=proxy_setting)
driver.get(url)
driver.maximize_window()
# driver.implicitly_wait(10)

top_group_list = driver.find_elements_by_css_selector('.id-cluster-container.cluster-container.cards-transition-enabled')
driver.get_screenshot_as_file('top.jpg’)
for top_group in top_group_list:
    group_name = top_group.find_element_by_xpath('div/div[@class="cluster-heading"]/h2/a').text
    for item in top_group.find_elements_by_class_name('title'):
        print u'bound: {0} app: {1}'.format(group_name,item.text)

driver.quit()
5. 爬取全站爬取一号店手机类目下的手机型号、价格。通过商品分类获取起始url: "http://list.yhd.com/c23586-0/?tp=15.53938003.561.0.3.LFfY%607f-10-Enf8s&ti=1FM9" ，从起始页中获取每个商品的url，继而抓取类目下的所有单品。爬取全站，网页较多，涉及到并发问题，此时可以使用Scrapy框架了。该类目下总共50页，拼接上页码，url格式如下：因为页码是用 # 拼接，会被Scrapy认为是同一个url，所以只会处理一次。使用浏览器访问url后观察，页面被重定向，真正的url格式却是这样的：我们可以使用重定向后的url作为种子url进行爬取，Scrapy代码如下：# -*- coding:UTF-8 -*-

import re
import time

from scrapy import Spider, Request
# from selenium import webdriver
# from selenium.webdriver.common.action_chains import ActionChains


class YhdMobileSpider(Spider):
    name = 'yhd_mobile'
    start_urls = ['http://list.yhd.com/c23586-0-81436/b/a-s1-v4-p1-price-d0-f0d-m1-rt0-pid-mid0-k/']

    def parse(self, response):
        '''
        @param response:
        @return: item list
        '''
        page_number = self.get_page_count(response)
        page_url_list = [ re.sub(r'-p[\d]+-', '-p{0}-'.format(page), response.url) for page in xrange(1, page_number+1) ]

        return map(lambda url: Request(url, callback=self.parse_product_page), page_url_list)


    def parse_product_page(self, response):
        product_url_list = []
        for product_address in response.xpath('//div[@id="itemSearchList"]/div/div[@class="itemBox"]/p[@class="proName clearfix"]/a[1]/@href'):
            href = product_address.extract()
            product_url_list.append(href)

        item_list = map(lambda url: Request(url, callback=self.parse_item), product_url_list)

        return item_list


    def parse_item(self, response):
        '''
        根据单品链接抓取单品的属性
        @param response:
        @return: item
        '''
        #商品详情地址
        url = response.url
        #品牌
        brand = response.xpath('//div[@class="crumb clearfix"]/a[@id="brand_relevance"]/text()').extract()[0]
        #商品名称
        spu_name = response.xpath('//div[@class="crumb clearfix"]/span/text()').extract()[0]

        print url,brand,spu_name


    def get_page_count(self,response):
        page_count = response.xpath('//input[@id="pageCountPage"]/@value').extract()
        if page_count:
            page_count = int(page_count[0])
        else:
            page_count = 1
        return page_count
几个例子都是之前工作的真实需求做的一些简化，后续再做适当变换扩展就可以进行复杂的爬取了，至于爬取之后的数据如何存储，做什么用，那又是另外一个话题了。（妈的，编辑好多遍知乎就是不能好好的显示 url 原地址，真是日了狗了，算了 将就着看吧）——————————————分割线——————————————刚看到一篇专栏，资源挺全的，新手要学习 Python 的可以关注一下：如何学习Python爬虫[入门篇]？ - 学习编程 - 知乎专栏


-------------------------answer 8 via  -------------------------


从入门到精通：如何入门 Python 爬虫？ - 谢科的回答


-------------------------answer 9 via  -------------------------


最近都在做爬虫相关，做到最后，做的最多的，就是攻克那些验证码，和数据经过怎么处理得到。还是那句，多动手，很多问题你不动手，你不知道其中的难度。
