


想知道大家都用python写过哪些有趣的脚本? - Python - 知乎






--------------------Link http://www.zhihu.com/question/28661987 ----------------------





--------------------Detail----------------------

最近学习python，据说很好玩，看着有些大牛写的脚本想学，但是还没什么方向，想知道大家都用python做过哪些事^_^

-------------------------answer 0 via  -------------------------


用python可以画画啊！可以画画啊！可以画画啊！ 对，有趣的事情需要讲三遍。事情是这样的，通过python的深度学习算法包去训练计算机模仿世界名画的风格，然后应用到另一幅画中，不多说直接上图！这个是世界名画”毕加索的自画像“（我也不懂什么是世界名画，但是我会google呀哈哈），以这张图片为模板，让计算机去学习这张图片的风格（至于怎么学习请参照这篇国外大牛的论文http://arxiv.org/abs/1508.06576）应用到自己的这张图片上。结果就变成下面这个样子了结果就变成下面这个样子了咦，吓死宝宝了，不过好玩的东西当然要身先士卒啦！接着由于距离开学也越来越近了，为了给广大新生营造一个良好的校园，噗！为了美化校园在新生心目中的形象学长真的不是有意要欺骗你们的。特意制作了下面的《梵高笔下的东华理工大学》，是不是没有听说过这个大学，的确她就是一个普通的二本学校不过这都不是重点。左边的图片是梵高的《星空》作为模板，中间的图片是待转化的图片，右边的图片是结果这是我们学校的内“湖”（池塘）这是我们学校的内“湖”（池塘）校园里的樱花广场（个人觉得这是我校最浪漫的地方了）校园里的樱花广场（个人觉得这是我校最浪漫的地方了）不多说，学校图书馆不多说，学校图书馆“池塘”边的柳树“池塘”边的柳树学校东大门学校东大门学校测绘楼学校测绘楼学校地学楼学校地学楼为了便于观看，附上生成后的大图：别看才区区七张图片，可是这让计算机运行了好长的时间，期间电脑死机两次！好了广告打完了，下面是福利时间基于python深度学习库DeepPy的实现：GitHub - andersbll/neural_artistic_style: Neural Artistic Style in Python基于python深度学习库TensorFlow的实现：GitHub - anishathalye/neural-style: Neural style in TensorFlow!基于python深度学习库Caffe的实现：https://github.com/fzliu/style-transfer我们真的都是站在巨人的肩膀上啊！！！！


-------------------------answer 1 via  -------------------------


高三的时候用塞班手机，为了督促自己学习，写了个带界面的高考倒计时工具。记得塞班系统有个记录菜单图标和对应的程序入口信息的xml文件，于是我给倒计时工具加了个功能，在上课的时候把这个xml文件读到内存后删掉对应文件，下课的时候再写回去。上课从此没有玩过手机。


-------------------------answer 2 via  -------------------------


写过一个去网站查询图书分类的脚本。开始使用Python时，主要用来做操作系统的自动化检查工作，基本都是获取系统信息，然后对字符串进行判断处理。后来有次我哥的学校来了15w的图书，要把这些图书分别分类放入图书馆，但是随书发来的Excel中书名、作者、ISBN这些信息都有，唯独缺少图书对应的分类，要是一本本去查询分类再贴标签，全校老师一起上都要做好久，就问我有没有办法给搞一个自动获取分类的程序，开始想用搜索引擎来做，实际发现搜索出来的结果很难分析，去豆瓣图书看里面的信息也不是很全，找了很久，发现了一个很兴奋的网站  OpenISBN： http://openisbn.com/ ，根据ISBN就能查到图书的所有信息，于是决定通过这个网站来完成。主要用到了 xlrd、xlwt、urllib、 BeautifulSoup、re这几个库，xlrd、xlwt分别用来读写excel，urllib用来抓取网站信息，BeautifulSoup 用来定位查找的范围，re则用来提取分类信息。程序写完自动跑了几个小时，把所有图书分类设置完成。（开始很担心网站会不会封我IP，然而网站良心大大的，并没有做任何处理，赞一个）


-------------------------answer 3 via  -------------------------


12年我找工作的时候曾经用Python写了一个简历，然后就靠它换了下一份工作当时的代码中有一些个人信息，我稍加改动也尽量保持了当年的风格，重新上传了一个 github.com 的页面效果是这样的（是2张图片拼起来的）：PS: 如果你特别懒，可以打开 PS: 如果你特别懒，可以打开 Your Python Trinket 看效果


-------------------------answer 4 via  -------------------------


学校隶属国家电网，有次系主任找我帮忙把40多个中层领导的信息（分散在n个Excel和一个数据库中）按照要求合并到一个格式明确的Excel中，然后导入到国家电网的库里。领导的信息精确到了谁谁谁的女儿在哪上学是不是少先队员，谁谁谁的父亲以前的职业是啥现在在干嘛这些。如果手工的话可能需要一个月不止（给我了一个月时间，每天没课的时候在人事处做，人事处一个秘书和我一起），做完以后有一定“物质”上的感谢（就是有一个月的工资）。我在人事处做了几天以后发现手工效率太低而且错误率比较高，然后用Python写了一个程序自动读领导信息录到Excel，写程序加上后期的人工校对一共花了不到一周（5天，周末不算）。后果是，秘书很感激我，但是“物质”上的感谢就不见了。。。（P.S.人事处的人找系主任从计科系找人帮忙，感情人事处是这么理解计科系的！！！）-----------------------------------------------------------------------------------------------------03/18更——-----------------------------------------------------------------------------------------------------写过一个爬**图片的脚本，完整代码在我的博客里。主要是受室友之邀，看在他没有女朋友的份上，就帮他做了一个爬小黄图的脚本，不用再忍受**网站无止境的弹窗和广告。地址：用BeautifulSoup库爬糗百成人-----------------------------------------------------------------------------------------------------寒假回家以后，配合树莓派写过远控电热毯的脚本，具体是因为躺在沙发的时候不想去卧室开电热毯，然后就直接手机操纵电热毯了。（我很懒~~~~~）现在天热了，电热毯不需要了，也回了学校，打算用【树莓派+Python】再做点什么，暂时没有想好。


-------------------------answer 5 via  -------------------------


刚工作的时候喜欢一个妹纸，有一次妹纸女神参加了一个当时还很流行的晒照比赛的活动，那时候还是微博比较火的时候。上图是刚从某度找的一张，给大家一个直观感受，女神比这个不知美多少倍。那个投票网站没有做很复杂的认证，投票也不需要注册账户，所以很轻松抓包找到了投票post接口，开始写了一个脚本来投票。很快发现同一个ip只能投一次，这个难不住写过不少爬虫的我，电脑直连宽带，断开宽带连接然后再连接，就可以获得一个新的ip，并且在联通的ip池中重复的概率并不大。于是很快将女神妹纸的名次刷到了第一名，然后。。。就没有然后了。。。。。。


-------------------------answer 6 via  -------------------------


刚刚用上了，有个任务在excel里面导些用户数据进数据库表，刚好学了差不多一星期的python，用上了，哈哈，不过写得不好，字段直接写死了，凑合用#-*- coding: utf-8 -*-
#encoding=utf-8
import MySQLdb
import xlrd
#打开excel
data = xlrd.open_workbook('testpython.xls')
#根据名字拿到excel的某个表
table = data.sheet_by_name('Sheet1')
#行数
nrows = table.nrows
for rownum in range(1,nrows):
	row = table.row_values(rownum)
	print len(row)

	# 打开数据库连接
	db = MySQLdb.connect("localhost","root","","pythonmysql" )
	#链接资源
	cursor = db.cursor()
	
	# SQL 插入语句
	sql = 'insert into pyuser (username,password, email, qq) values("%s", "%s","%s","%s")' % \
	(row[0],row[1],row[2],row[3])
	print sql
	try:
	   # 执行sql语句
	   cursor.execute(sql)
	   # 提交到数据库执行
	   db.commit()
	except:
	   db.rollback()
	# 关闭数据库连接
	db.close()
算完成了，大神勿喷继续更新：喜欢切尔西，于是弄了一个sae，不过这一次，用pyquery+requests+lxml来搞了，每天新浪，网易，腾讯三大门户网站更新我车的文章，我就把它爬到我的sae的web上，地址：Chelsea新闻python脚本：163
 #-*- coding: utf-8 -*-
#encoding=utf-8
import MySQLdb
from pyquery import PyQuery
from time import ctime,sleep
import requests
import os
import sys 

reload(sys) 
sys.setdefaultencoding('utf-8')

def getNews(doc):
  for data in doc('.articleList li'):
    title = PyQuery(data).find('a').text()
    link = PyQuery(data).find('a').attr('href')
    addTime = PyQuery(data).children('.postTime').text()
    cname = '网易新闻'
    comeLink = 'http://www.163.com/'
    getData(title,link,cname,addTime,comeLink)

def getData(title,link,cname,addTime,comeLink):
    db = MySQLdb.connect(host="localhost", port=3306,user="root",passwd="123456",db="app_chelseafc",charset="utf8")
    #db = MySQLdb.connect(host="10.67.15.102", port=3307,user="5330x2woz0",passwd="5iihxiwxx4kjlim5kl4m14wmx115myw0y243y530",db="app_chelseafc",charset="utf8")
    cursor = db.cursor()
    sql = 'insert into acticle (title,link,cname,addTime,comeLink) values("%s","%s","%s","%s","%s")' % (title,link,cname,addTime,comeLink)
    print sql
    try:
       cursor.execute(sql)
       db.commit()
       print "success"
    except:
       db.rollback()
    db.close()
if __name__ == '__main__':
	r = requests.get("http://sports.163.com/special/y/00051F15/ycqexmore.html")
	html = r.text
	doc = PyQuery(html);
	getNews(doc)

qq：
 #-*- coding: utf-8 -*-
#encoding=utf-8
import MySQLdb
from pyquery import PyQuery
from time import ctime,sleep
import requests
import os
import sys 

reload(sys) 
sys.setdefaultencoding('utf-8')

def getNews(doc):
  for data in doc('.newslist li'):
    title = PyQuery(data).find('a').text()
    link = PyQuery(data).find('a').attr('href')
    cname = '腾讯新闻'
    comeLink = 'http://www.qq.com/'
    addTime = ctime()
    getData(title,link,cname,addTime,comeLink)

def getData(title,link,cname,addTime,comeLink):
    db = MySQLdb.connect(host="localhost", port=3306,user="root",passwd="123456",db="app_chelseafc",charset="utf8")
    #db = MySQLdb.connect(host="10.67.15.102", port=3307,user="5330x2woz0",passwd="5iihxiwxx4kjlim5kl4m14wmx115myw0y243y530",db="app_chelseafc",charset="utf8")
    cursor = db.cursor()
    sql = 'insert into acticle (title,link,cname,addTime,comeLink) values("%s","%s","%s","%s","%s")' % (title,link,cname,addTime,comeLink)
    print sql
    try:
       cursor.execute(sql)
       db.commit()
       print "success"
    except:
       db.rollback()
    db.close()
if __name__ == '__main__':
	r = requests.get("http://sports.qq.com/l/isocce/yingc/chelse/che.htm")
	html = r.text
	doc = PyQuery(html);
	getNews(doc)

sina：
 #-*- coding: utf-8 -*-
#encoding=utf-8
import MySQLdb
from pyquery import PyQuery
from time import ctime,sleep
import requests
import os
import sys 
reload(sys) 
sys.setdefaultencoding('utf-8')
def getNews(doc):
  for data in doc('.d_list_txt li'):
    title = PyQuery(data).find('a').text()
    link = PyQuery(data).find('a').attr('href')
    addTime = PyQuery(data).children('.c_time').text()
    cname = '新浪新闻'
    comeLink = 'http://www.sina.com.cn/'
    getData(title,link,cname,addTime,comeLink)

def getData(title,link,cname,addTime,comeLink):
    db = MySQLdb.connect(host="localhost", port=3306,user="root",passwd="123456",db="app_chelseafc",charset="utf8")
    #db = MySQLdb.connect(host="10.67.15.102", port=3307,user="5330x2woz0",passwd="5iihxiwxx4kjlim5kl4m14wmx115myw0y243y530",db="app_chelseafc",charset="utf8")
    cursor = db.cursor()
    sql = 'insert into acticle (title,link,cname,addTime,comeLink) values("%s","%s","%s","%s","%s")' % (title,link,cname,addTime,comeLink)
    print sql
    try:
       cursor.execute(sql)
       db.commit()
       print "success"
    except:
       db.rollback()
    db.close()
if __name__ == '__main__':
  r = requests.get("http://roll.sports.sina.com.cn/s_premierleague_all/3/index.shtml")
  r.encoding ='gb2312'
  html = r.text
  doc = PyQuery(html)
  getNews(doc)
真的很好玩有空再搞一个提醒功能，三大门户网站更新就提醒一下，再撸撸多线程


-------------------------answer 7 via  -------------------------


之前一直想回答这个问题 , 但是感觉一点资格也没有. 现在我的python终于用到了 , 让我万分激动 , 怒答这个问题.在我做了这件事情之后 , 她觉得我越来越懂她了 , 嘻嘻     有一天 , 我发现我心仪已久的妹纸在朋友圈里分享了知乎专栏的文章 , 就知道她也刷知乎 . 如果在知乎上关注她 , 我就能知道 , 她最近关注什么 , 心里想些什么 , 了解了解她喜欢的方面 , 还能作为谈资 , 简直太赞了 (*^^)v .      但是输入她的名字...... 在知乎上根本找不到好吗   (๑`灬´๑)  我们两个聊天的时候 , 刚好说到了她分享的那篇文章 ,        我很自然的说: "知乎上你用的不是真名呀, 就我这么天真用了真名.."        她笑着说:"那个可以改呀" ,       "凭什么知乎团队不让我改啊!!! ",我答道," 不如我们互粉吧^_- "         哎 , 于是她打开zhihu , 看了看我的主页 , 并没有关注我...... 可能是赞太少了吧... 达不到她的要求 , 或者她不想让我知道她在看什么吧 , 也许她希望自己的知乎是交浅言深 , 不希望被身边人看见... (๑-﹏-๑) 失望.      我回去想了想 , 她说名字可以改 , 那她可能以前也用的是真名 , 找到破绽了!    知乎的名字可以改 , 但是id是改不了的 !每个人的主页地址 , people后面那个就是TA的id, http://www.zhihu.com/people/zhang-san-12-45
例如张三同名很多 , 后面就会加上数字. 她的名字拼音相同的较多 ,  我试了一下 , 这个数字是不超过100的. 它的组合方式有 zhang-san , zhang-san-1 zhang-san-12-43 依次类推.好 , 现在我就可以开始寻找她的账号了!  既然她改名了 , 那她名字满足的条件一定是: 昵称的拼音不是真名. 这个用pypinyin模块可以解决 , 这样子 , 需要我人工查看的主页就少很多了.1. 在github上 下载 @egrcc 的zhihu-python2. 寻找她了ing# coding: utf-8

from zhihu import User
from pypinyin import pinyin, lazy_pinyin
import pypinyin

user_url = ''
user_id = ''
l = [u'bu', u'xu', u'kan']
 #这里是她名字的拼音, 还是不要暴露她的好, (*/ω＼*)
for num in range(100):  #先在 -100以内搜索
    try:
        user_url = 'http://www.zhihu.com/people/bu-xu-kan-' + str(num)
        user = User(user_url)
        user_id = user.get_user_id()
        if l != lazy_pinyin(user_id.decode('gbk')): #看看她有没有用原名
            print user_id, ' ', num
    except:
        pass

for i in range(100):   
    for j in range(100):  #在 -100-100以内搜索
        try:
            user_url = 'http://www.zhihu.com/people/bu-xu-kan-' + str(i) + '-' + str(j)
            user = User(user_url)
            user_id = user.get_user_id()
            print user_id, ' ', i, '-', j
        except:
            pass
爬了好久 , 结果出来了 , 这些昵称不多 , 我翻翻他们的主页就幸运地找到了我心仪的妹纸:XXXXXXXX   26
XXXXXXXX   27
XXXXXXXX   42
XXXXXXXX   72
XXXXXXXX   94
she is here!   6 - 36
XXXXXXXX   6 - 76
XXXXXXXX   7 - 86
XXXXXXXX   10 - 35
XXXXXXXX   28 - 67
XXXXXXXX   32 - 28
XXXXXXXX   32 - 66
XXXXXXXX   34 - 75
从那之后 , 我每天都可以看她的主页啦~ 至于我有没有追到她呢....----------------------------------------------------------------------------------------------------------------我匿名的原因是因为我正在追她 , 如果我追到 , 或者没追到她 , 我就不匿了. 在我打开她的主页之后 , 我发现她喜欢科幻 , 也对推理小说感兴趣 , 关注穿衣打扮方面 , 符合我的胃口呀 . 最近呢 , 她关注情感方面的问题变多了 , 我不知道是不是因为最近我和她联系变频繁了 , 激起了她一些感觉 , (*/ω＼*)我会加油哒~


-------------------------answer 8 via  -------------------------


一个自动登录小木虫并领取金币，半夜免费下载小木虫付费资源的python脚本。你电脑上跑着哪些好玩脚本？ - 邵正将的回答


-------------------------answer 9 via  -------------------------


用nmap花了三天时间扫描学校内网所有主机80端口，用Python正则对结果进行过滤找到80端口open的主机，手动试出了某个摄像头终端的ip，随机试出用户名密码，提取关键信息，用Python对上面找到的所有ip进行模拟登陆抓取页面并匹配关键字，找到了学校所有监控的入口。。。然后。。就没有然后了。。果断匿了===============================================居然被学弟认出来了，就不匿了吧
