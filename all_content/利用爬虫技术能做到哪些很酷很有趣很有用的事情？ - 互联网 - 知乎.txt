


利用爬虫技术能做到哪些很酷很有趣很有用的事情？ - 互联网 - 知乎






--------------------Link http://www.zhihu.com/question/27621722 ----------------------





--------------------Detail----------------------

准备学习python爬虫。各位大神都会用爬虫做哪些有趣的事情？今天突然想玩玩爬虫，就提了这个问题。跟着YouTube上的一个tutor写了个简单的程序，爬了一点豆瓣的数据。主要用到request和bs4（BeautifulSoup）模块。虽然简陋，毕竟是人生中的第一只爬虫啊……以示纪念，代码写在博客里了：我的第一只爬虫：爬取豆瓣读书

-------------------------answer 0 via  -------------------------


谢邀.2011年夏天我在google实习的时候做了一些Twitter数据相关的开发，之后我看到了一片关于利用twitter上人的心情来预测股市的论文(http://battleofthequants.net/wp-content/uploads/2013/03/2010-10-15_JOCS_Twitter_Mood.pdf)。实习结束后我跟几个朋友聊了聊，我就想能不能自己做一点twitter的数据挖掘，当时只是想先写个爬虫玩玩，没想最后开发了两年多，抓取了一千多万用户的400亿条tweet。上分析篇先给大家看一些分析结果吧。大家几点睡觉呢? 我们来统计一下sleep这个词在twitter上出现的频率。看来很多人喜欢在睡前会说一声我睡了。那我们再看一个更有意思的 :"Thursday"这个词的每天出现的频率。这里2月2号是周四，不出意料，这一天提到周四的频率最高。而且好像离周四越近这个频率越高。可是，为什么2月1号的频率反而低了呢？是因为2月1号大家不说周四而说明天了（有的人会说2月2号也可以说是今天，但是因为在2月2号提到当天的次数太高，因此还是有很多人用周四这个词）。做了词频统计我们还可以做一些语义分析。我们可以利用unsupervised learning来分析一条tweet的感情色彩。我们对每一条tweet的高兴程度在0至1之间打分，并对每天做平均值，就得到了下面这张图。这里最明显的特征恐怕就是周期性了。是的，大家普遍周末比较高兴。不过这张图的开始和中间有两个点与周期不吻合。如果我告诉你这两天是1月1日和2月14日，那你肯定会想到为什么了，元旦和情人节很多人是很高兴的（不排除slient majority存在的可能）。这很有意思，但似乎没什么用啊。那我们来看下面这张图，还是2012年的情感分析，不过这里对用户进行了过滤，只保留了来自投资人和交易员的tweet （根据用户的tweet我们可以估计他/她的职业）。蓝线是这些用户的感情色彩，红线是S&P 500指数。看来行情好的时候大家都高兴啊。最后我们再来看两个统计图吧。2012年是美国大选年，这里统计了在所有和奥巴马相关的tweet里跟提到经济的tweet占的比例。红线是这个比例，黑线是S&P 500貌似和美国经济有负相关性啊！为什么呢，我们看下面的图就明白了。这个比例和美国失业率正相关，而经济和失业率又是负相关的。换句话说，美国人（尤其是共和党的）找不到工作了就开始埋怨奥巴马了。除了上面的分析外我做了很多其他的研究，比如如何判断一个用户的职业，验证六度分隔理论, 以及网络扩张速度的建模，不过这里就先不赘述了。最后要说的是以上的分析在统计上都是不严谨的，twitter上的信息杂音非常大，又有很强的demographic bias，有很多因素都没有考虑。我们只能希望大数定律能过弥补一些误差。写在这里只是抛砖引玉，给大家看一下爬虫可以做什么。大家感兴趣的话之后我可以补充一下这两个话题：1. 怎样判断一条tweet的感情色彩2. 怎样估计一个twitter用户的职业下技术篇 当时Twitter用户大概已经有上亿了，每天新的tweet也有几千万甚至上亿。能不能把这些数据全部抓取下来呢？这是可能的。Twitter是有API的，不过每个IP地址每小时可以抓取150个用户最近的tweet，以这个速度要把几亿个用户抓取一遍需要近一百年。但是，大部分Twitter用户是不活跃甚至从来不发tweet的，还有很多用户是印尼等国家（不是他们不重要，我真的看不懂他们发的tweet），如果我们把不说英语，不发tweet以及follow人数不超过5个（好像注册twitter后用户会被要求follow 5个人）的用户过滤掉，我们就剩下了大约10,000,000个用户，十年就可以搞定了。十年好像还是太长了。。。不过twitter的访问限制是基于IP地址的，只要我从多个IP访问twitter不久好了(我真的没有DDOS twitter的意思啊)？那么下一步就是搜集大量代理服务器来访问twitter api。为了做twitter的爬虫我专门做了一个爬虫去搜集免费代理服务器。免费的东西总是有代价的，这些服务器非常不稳定。因此我又建立了一套代理服务器管理系统，定期更新IP地址，删除不能用的服务器。最后这套系统平均每天有几百个可用的服务器，大约半个月就可以把一千万个用户抓取一遍了。此外我又做了一些动态优化，根据twitter用户的follower数量决定他们的抓取频率，以提高重要用户tweet的实时性。在一年半的时间里，这套系统一共抓取了400亿条tweet，加起来得有10TB，估计占来自美国tweet数量的一半左右。那么问题来了，怎么存贮这些tweet呢？如果要做分析的话恐怕把数据读一遍就要好几天了。很多人马上会说hadoop, cassandra, spark等等。不过作为一个穷学生我哪里有钱去做一个cluster呢？这些数据存在AWS上就得每月1000刀了。自己动手，丰衣足食。解决方案就是自己组装一个服务器，买了8块3T硬盘做了一个12TB的磁盘矩阵放在寝室里。软件使用了最为传统的MySQL，这是一个存了400亿条数据的MySQL数据库。我花了大量时间去做优化，尝试了各种各样的partition, ordering, indexing。最后可以实现一天之内对100-200亿条数据进行线型搜索或过滤，或者几秒钟内调取某一天的或某一条tweet。这台服务器现在留在了MIT，毕业后我把它提供给了一位教授做研究。PS:这个项目在2013年停止了，因为social media已经不在火，而且twitter于2013年中关闭了相关的API接口。这个项目的初衷是学术性质的，我不想违反twitter的服务条款，因此这些数据没有被出售或者用来谋求商业价值，而是留给了MIT做研究。在这期间与几个朋友进行了很愉快的合作，未征得他们允许就不在此提名了。暂时没有开源的打算，因为当时水平有限，代码写得太丑了（用java写的）。PS2：很多人问怎么找代理服务器，请大家google一下吧。当然如果不能翻墙的话有代理服务器恐怕也不能用。谢绝转载。


-------------------------answer 1 via  -------------------------


可以带逛呀！爬了知乎12万用户的头像，把长得像的头像放在一起，方便浏览：http://lab.grapeot.me/zhihu/touxiang/bai-yuan-yuan-73.html然后搜集了知友们的点击，预测出来这是你们（平均）最喜欢的人长的样子：然后根据点击数据训练出来了一个带逛机器人，可以自动识别美女：http://lab.grapeot.me/zhihu/autoface更详细的信息可以参见我的专栏文章：带逛传万世 因有我参与 - 挖掘知乎里有趣的东西 - 知乎专栏你们最爱的知乎头像 - 挖掘知乎里有趣的东西 - 知乎专栏头像带逛 - 挖掘知乎里有趣的东西 - 知乎专栏


-------------------------answer 2 via  -------------------------


我当初是看到这个帖子才知道Python这门语言的功能，才开始去学的，现在也学了一小段时间。不得不说，Python爬虫对于我来说真是个神器。之前在分析一些经济数据的时候，需要从网上抓取一些数据下来，想了很多方法，一开始是通过Excel，但是Excel只能爬下表格，局限性太大了。之后问了学编程的朋友，他说JavaScript也能实现，于是懵懵懂懂地就去学Java（我那朋友是学Java的，我当时问他用Java能不能实现，他说JavaScript好像可以，当时我什么都不懂，就把JavaScript理解成是Java下的一个分支，以为JavaScript只是Java其中一个包什么的，于是我便去学了一小会Java，无知惹的祸啊。。。）。但整个Java体系也太庞大了，学起来力不从心，毕竟我只是要运用其中一部分功能而已，于是学没多久我就放弃了。就在我迷茫的时候，我发现了Python大法……        废话说多了，说说自己的学习经历吧。也给想学Python，想写爬虫的人一个参考。        一开始我是在网易云课堂上自己找了个基础的视频来学，Python是真是门简单的语言，之前懂一点Visual Basic，感觉Python也很适合给无编程基础的人学习。        入门视频到最后，就做出了我的第一个爬虫——百度贴吧图片爬虫（相信很多的教程都是以百度贴吧爬虫为经典例子来说的。）        一开始代码很简单，只能爬取第一页的数据，于是我加了一个循环，就能够爬取制定页数的图片了。并且图片是有按顺序排列的，非常方便。在筛选网址的时候用正则表达式就好了。正则表达式使用：[精华] 正则表达式30分钟入门教程        可是我不经常混贴吧啊，也很少有要下载贴吧图片的需求。回归初衷吧。我对投资有兴趣，学编程有一个原因也是为了投资服务。在7月股灾进行时的时候，我错过了一个明显的“捡钱”的机会，并非自身专业知识不够，而是当时在准备考试，很少去看股市，这让我心有不甘：要是有个东西能够帮我自动爬取数据分析并推送就好了，于是有了以下学习轨迹：一、爬取数据        在此顺便贴上一个Python系列教程http://www.jikexueyuan.com/path/python/，极客学院，里面有些教程还是挺不错的。从里面我知道了两个可以替代Python里urllib和re正则表达式的库，它们分别叫做requests和lxml。        第一个库挺不错的，现在在获取网页源代码时，我都用这个库，大家如果有不懂的可以看看那个网站。第二个库由于我是用3.4版本的Python，折腾了很久没折腾进去，于是我发现了另一个不错的库BeautifulSoup，详细教程参考：Python爬虫入门八之Beautiful Soup的用法        有了requests和Beautifulsoup，基本上可以实现我想要的很多功能了。我便做了一个抓取分级基金数据的爬虫： 二、分析并推送         其实在此分析其实还谈不上，顶多算是筛选。（不过我相信随着我数学能力提升会能有进一步的分析的，美好的祝愿。。。）筛选很简单，就是涨幅或收益率等等满足一定条件就保留下来，保留下来干嘛？推送啊！！！         将保存下来的数据通过邮件发送到自己的邮箱，手机上下载个软件，一切就大功告成了！         至此当时学习Python的目的就达到了，当时鸡冻地要炸了！！！        不过……那么好玩的东西，怎么能这么快就结束了？再折腾吧！于是三、简单的界面       等等！Python好像不能直接弄成exe可执行文件，不能每次运行都开Python的窗口啊！强迫症怎么能忍！上网搜搜发现有诸如py2exe的包可以转换，可是老子是3.4版本啊！折腾半天没搞定，算了！我不是会点VB吗，用那个吧。于是连界面都有了        刚好会点PS，做做低级的界面也不错。四、云服务器做完界面我以为就结束了，我还是too young啊。用了几天发现，我总不能天天开着电脑让它就运行那么几个程序吧？总得有个地方能让我24小时运行这些程序。本来想跟朋友的电脑轮流运行，还是太麻烦。偶然的机会我发现了云服务器这个东西。了解后砸下重金买下服务器（其实一个月30而已……）        折腾一番linux系统的操作，实现了24小时的实时推送。        折腾一番linux系统的操作，实现了24小时的实时推送。        而到这里，我已经深陷到Python里了，我觉得我应该继续学习这门强大简单的语言，在知乎上看到了一个问题：Quant 应该学习哪些 Python 知识？ - 薛昆Kelvin 的回答，虽然说的是Quant但也为我指引了一些方向。目前正准备学习numpy,pandas,matplotlib这些库，以实现未来对金融、经济数据的可视化和分析。相关的内容有一本书写得还不错，叫《利用Python进行数据分析》，有兴趣学习可以读一读。        共勉。—————————2015.9.23更新—————————好多人问是什么服务器，我用的是阿里云。有些人反应价格很高，其实把配置调到最低，可以满足基本需求，价格只要30左右。正好刚刚收到一封邮件，学生党有福利了（我真的不是在打广告啊……）—————————2015.10.2更新—————————快破千赞了，有点出乎意料，补充几点吧。1.我用阿里云发现最低的也要100/80/40（各种价格），答主你不是在骗我吧？直接上图，32元左右。直接上图，32元左右。其实国内的服务器有挺多的，一个月三四十的低配很多地方都买得到。不过评论区有人提到一些外国个人的云服务器价格有些在30~50/年，很便宜。答主暂时还没去看，因为买的服务器还没过期，有需要的可以翻翻评论区看看。（另外，亚马逊好像有免费一年的云服务器试用。）2.Python3也可以转成exe我只是按照自己的学习轨迹写的回答，当初是在不知道云服务器的情况下才有转化成exe的需求，并且当时了解得不多，屡屡碰壁没能完成。现在已经不需要了，不过还是谢谢大家的提醒。这里顺便提醒一下，各位初学Python务必装入pip，不要像我一样怕麻烦，结果导致一些库花了好长时间才折腾进去，其实只要“pip install XXX”就很轻松搞定了。3.从哪里爬来的数据？见另一个回答：有哪些网站用爬虫爬取能得到很有价值的数据？ - 林骏翔的回答


-------------------------answer 3 via  -------------------------


该轮到我祭出我当年研究生期间在实验室里参与或旁观的各种有用或者有趣的课题了：1. 建立机器翻译的语料库。这是我研究生期间的核心课题，我先来介绍下背景。大家其实都用过谷歌翻译、百度翻译，虽然确实槽点很多，但不妨碍机器翻译相较过去已经达到基本可用的程度了。我大概说下机器翻译的原理。在几十年前，计算机学家们的思路是，既然是人工智能的范畴，就让计算机懂得语法规则、知道词语含义，跟小孩子上学时学习的语言课程一样去做训练，就应该可以了。但结果是，基于语义和语法规则的机器翻译效果糟糕得一塌糊涂。究其原因，还是每个词语的含义实在太多、每句话的语境不同意思也会不同，更别说不同语言中要表达清楚同一个意思的方式也完全不同。比如下图这个，你觉得英语国家的人能看懂吗：其实，当时也有另一派，叫做统计派。他们认为，就跟当年战胜国际象棋世界冠军的“深蓝（深蓝（美国国际象棋电脑））”一样，应当用统计的方式去做。大家知道，“深蓝”并没有领会象棋的下法，而只是熟悉几百万的棋局，懂得怎样走从概率上看起来是最正确的。机器翻译也是这样，完全可以输入人工翻译的大量语料，然后做出统计模型，让计算机尽可能地熟悉别人是怎么翻译的，从而耳濡目染，也能“假装”可以翻译了。但那个年代并没有条件收集大量语料信息。后来很多年后，谷歌出现了，随之出现的还有它的超大数据规模和超强的计算能力，于是谷歌的统计机器翻译系统也就是全球正确率最高的系统之一了。而目前你所用过的、见到的机器翻译工具，全都是用的统计方法。故事大概就是这样。目前学术界的机器翻译方法中，统计机器翻译基本是垄断的地位。而效果的好坏，则基本就看语料库的规模。（想了解更多，推荐阅读 数学之美 (豆瓣) 的第2章“‘自然语言处理 — 从规则到统计”及第3章“统计语言模型”）所以你知道了，我的任务就是跟同学做一个爬虫和简易的分析系统，从而建立一个大规模的语料库。网上双语的资源还是挺多的，大都像这种：我们的爬取步骤大概是：1. 对当前网页进行简易判断，如果有双语嫌疑，则收录整理出来双语的正文；如果没有，弃用；2. 将正文内容进行详细判断，确定是双语文本，则进行段落对齐和句子对齐，整理到语料库；如果没有，弃用；3. 对当前网页的所有链接网页，重复步骤 1有详细介绍的我们申请的专利在这里：http://www.soopat.com/Patent/201210442487其实我们当时的双语判断算法和对齐算法这些都不是难点，难点在机器配置、爬虫设计和服务器维护上。我们几乎天天流窜在机房（配置机器、接线、装机）、实验室（编写、运行代码）、网络中心（跪求带宽）、学校物业（空调他妈又坏了）这几个地方，总是没法消停。最痛苦的是，假期里回家远程访问下爬虫，发现 down 机了... 整个假期的宝贵时间就浪费了。这是我们当时在又闷又热又吵的机房的照片：好在最后我们终于爬到了要求的语料规模，并且通过了国家项目的验收。现在这些包括中英俄日的庞大语料正在百度翻译中起到重要的作用，如果你用过百度翻译，不妨给我点个感谢 ^_^如果你对机器翻译感兴趣，也可以自己爬点双语语料，做个翻译器玩玩。这是一个极其简易的搭建教程：机器翻译系统的搭建。可以用它介绍的 1500 句对，也可以自己多爬一些。搭建好之后，你输入一句话，看到机器像模像样地回一句半生不熟的翻译，还是有可能会被萌到的。当然，要是你希望像我们一样搭建千万级甚至亿级的语料库，并且做一个翻译器，那你需要有特别强大计算能力和存储能力的服务器、非常宽的带宽，以及强大的耐心和毅力...2. 社会计算方面的统计和预测很多朋友已经提到了可以通过爬虫得到的数据做一些社会计算的分析。我们实验室爬取了大规模的新浪微博内容数据（可能是非商用机构中最多的），并针对这些数据做了很多有趣的尝试。2.1 情绪地图@Emily L 提到了著名的根据情绪预测股市的论文：http://battleofthequants.net/wp-content/uploads/2013/03/2010-10-15_JOCS_Twitter_Mood.pdf 。其实我们也仿照做了国内的，不过没有预测，只是监测目前微博上大家的情绪，也是极有趣的。我们把情绪类型分为“喜悦”“愤怒”“悲伤”“恐惧”“惊奇”，并且对能体现情绪的词语进行权重的标记，从而给每天每个省份都计算出一个情绪指数。界面大概是这样：可以直观看到全国各省份的情绪。不过实际上我感觉省份的区别不明显，最明显的是每天全国情绪指数的变动。比如春节是 2 月 18 日，那天的情绪指数是 41.27，前一天是 33.04，后一天则是 39.66。跟除夕夜都在吐槽和抱怨春晚，而大年初一则都在拜年情绪高涨，初二有所回落这样的状况预估是一致的。比如今年 1 月 2 日，上海踩踏事故开始登上各大媒体头条，成了热点话题，整个微博的情绪指数就骤降到 33.99 。再比如 5 月份情绪指数最高的是 5·20，因为今年开始流行示爱和表白；其次就是五一假期那几天。同样跟现实状况的预估是一致的。访问地址：http://123.126.42.100:5929/flexweb/index.html2.2 饮食地图我们抽取出所有美食相关词语，然后基于大家提到的美食次数，做了这么一份饮食地图。你可以查看不同省份、不同性别的用户、不同的时间段对不同类别食物的关注程度。比如你可以看到广东整体的美食关注：还可以把男的排除掉，只看女的：还可以具体到，看广东女性每天早上会提到什么喝的：访问地址：微博用户饮食习惯分析2.3 票房预测这是我们实验室最大胆的尝试，希望利用微博上大家表现出来的，对某部电影的期待值和关注度，来预测其票房。细节就不介绍了，目前对某些电影的预测比较准，某些则差很多。因为显然，很多电影是大家不用说也会默默买票，而很多电影是大家乐于讨论但不愿出钱到电影院去看的。界面是这样的：访问地址：电影票房预测-SCIR最后贴上我们实验室的官方网站：哈尔滨工业大学社会计算与信息检索研究中心3. 写在后面现在国内的社交平台（微博、豆瓣、知乎）已经积累了很多信息，在上面可分析的事情太多啦。大到政府部门需要的舆情监控，小到可以看看喜欢的姑娘最近情绪如何。其中有些会特别有价值，比如一些重要的预测（股市预测、票房预测），真的做成了的话商业价值根本不可估量；还有些会特别有趣，比如看看 5·20 的时候大家最爱说的情话是什么，看看我跟李开复之间最近的关系链是什么。说到这，我突然很想做个知乎的分析。在内容方面，比如看看全知乎的文字里最常出现的人名到底是 @张佳玮还是 @梁边妖；比如看看政治或者历史话题下，以表达情绪的词作为依据，大家的正能量多还是负能量多；比如看看当大家提到哪些东西时，情绪会最激动（中医？老罗？穆斯林？）。在关系方面，比如看看我的朋友、传说中认识所有知乎女 V 的谁谁谁到底还有哪个女 V 没有关注；比如看看知乎有哪些社交达人，虽然没多少赞但关系链却在大 V 们中间；比如看看有没有关注了我同时还被 @朱炫 关注的，这样我可以托他给我介绍大师兄。有没有人一起来嗨？让我看到你的手！


-------------------------answer 4 via  -------------------------


彩蛋：彩蛋已关闭骚瑞为后来的同学解释一下彩蛋怎么回事，顺便对昨晚12点之后收不到彩蛋的同学抱歉（鞠躬），被屏蔽了彩蛋是如果赞了这条答案会自动收到一条随机的私信，里面是一则短笑话笑话是在某网站上爬下来的，一共几十条随机发送起因是昨天写完原答案，突然想到如果加上彩蛋会不会很多人点赞（说我不是骗赞自己也不信）于是写了个小脚本，跑了起来试了一下第一次高潮出现在回答完30分钟后，突然多了一两百的赞，由于私信发送时间间隔太短，挂掉了修复后坚持到了晚上十二点，本机和VPS都不能再持续发送私信，于是停掉了今早起来发现赞又多了3000，崩溃的我决定还是不接着发了。。。代码和逻辑如下：// 代码不全，只有主要的逻辑
// 用到的库如下：

var request = require('superagent');
var cheerio = require('cheerio');
var fs = require('fs');

// 首先是这样的一个接口，可以取到某个答案所有赞同的人数
// 每次取会返回10条数据，是编译好的HTML模版，还有下一组数据的地址
// 遍历这10条数据并取到所有人的ID即可
// config 是Cookie、Host、Referer等配置

var sourceLink = 'https://www.zhihu.com/answer/' + code + '/voters_profile';

function getVoterList(link, fn) {
var next = '';
if (postListLength && !sleepIng) {
console.log('waiting');
sleepIng = true;
return setTimeout(function () {
sleepIng = false;
sleep = 1;
getVoterList(link, fn);
}, 1000 * 60);
}
request.get(link)
.set(config)
.end(function (err, res) {
if (err || !res.ok) {
return console.log(err);
}
var result = JSON.parse(res.text), voterList = '', $;

if (result.paging && result.paging.next) {
next = result.paging.next;
}

if (result.payload && result.payload.length) {
voterList = result.payload.join('');
$ = cheerio.load(voterList);

$('.zm-rich-follow-btn').each(function () {
var id = $(this).attr('data-id');

if (voterIdList.indexOf(id) === -1 && oldIdList.indexOf(id) === -1) {
console.log('new id: ', id);
voterIdList.push(id);
} else {
dupIdLen += 1;
}
});
}

if (next && dupIdLen < 20) {
setTimeout(function () {
getVoterList('https://www.zhihu.com' + next, fn);
}, 3000);
} else {
dupIdLen = 0;
fn();
}
});
}

// 在爬取完该接口后，新的点赞人数会暂存在数组中，遍历该数组，并发送请求
// 如请求发送成功，将各ID保存在某一个文件中，如发送失败，等几分钟后重试

function sendPost() {
var hasError = false;
var tempArr = [];
postListLength = voterIdList.length;
console.log('send post');

if (voterIdList.length) {
voterIdList.forEach(function (id, i) {

if (hasError) {
// 处理发送失败的情况，等待5分钟重试
if (!sleepIng) {
console.log('waiting');
sleepIng = true;
return setTimeout(function () {
sleepIng = false;
sleep = 1;
sendPost();
}, 1000 * 60 * 5);
}

return console.log('has error');
}

var index = (function () {
return i;
})(i);
var postIndex = index > postList.length ? index % postList.length : index;

setTimeout(function () {
// 一波发送完成之前不会启动下一波私信发送
postListLength--;
request.post('https://www.zhihu.com/inbox/post')
.send({
member_id: id,
content: postList[postIndex],
token: '',
_xsrf: '' // 这里是发送者的Cookie
})
.set(config)
.set({"Accept": "*/*"})
.set({"Content-Type": "application/x-www-form-urlencoded; charset=UTF-8"})
.end(function (err, res) {
console.log('hasError: ', hasError);
console.log(new Date());
console.log(res.text);
var resObj = {};

try {
resObj = JSON.parse(res.text);
} catch (e) {
console.log(e);

if (!sleepIng) {
hasError = true;
sleep = 5;
console.log('waiting');
sleepIng = true;
return setTimeout(function () {
sleepIng = false;
sleep = 1;
sendPost();
}, 1000 * 60 * 5);
}
}

if (err || !res.ok || resObj.r !== 0) {
console.log(err);
hasError = true;
sleep = 5;
tempArr = voterIdList.slice(0, index);
oldIdList = oldIdList.concat(tempArr);
fs.writeFile('./idlist.json', oldIdList, function (err) {
if (err) console.log(err);
});
}
});
}, 20 * 1000 * index * sleep);

if (index === voterIdList.length - 1) {
console.log('last');
oldIdList = oldIdList.concat(voterIdList);
voterIdList = [];
setTimeout(function () {
console.log('run again');
getVoterList(sourceLink, sendPost);
}, 1000 * 60 * 15);

fs.writeFile('./idlist.json', oldIdList, function (err) {
if (err) console.log(err);
});

console.log('done ');
}
});
} else {
setTimeout(function () {
console.log('run again');
getVoterList(sourceLink, sendPost);
}, 1000 * 60);
}
}
代码花了半个小时写的，比较糙，不过跑了一下确实能用，既然已经不发了就不改了，有同学要求就发上来了PS 知乎的策略应该有变化，昨晚12点之前只要对同一个人两条私信不重复，把握好发送时间间隔就没问题，12点之后我的VPS已经不能用了，时间间隔再久也会返回500错误，1点后我的本机也不行了，不断的返回500和403，Cookie也有更新，索性就停掉了这是昨晚爬到的ID还有我的视角所看的我的私信列表＝ ＝就酱＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝某人有一天书荒了，想要看豆瓣上的高分书，然而豆瓣并没有提供按评分的检索，于是拜托我写一个小东西，要求是能按现有标签来分类检索豆瓣图书，并按分数从高到低排序需求不难，就是数据没有，于是写了个爬虫按标签爬下来豆瓣所有的书爬的时候只爬了分类的列表，这样有书籍的名称，链接，评分，分类，够用了，而且一次请求可以拿到较多的数据，并发不高的情况下能较快的爬完豆瓣所有的书爬数据的时间大概两个多小时左右，每次请求间隔3秒，倒是没被屏蔽代码用node写的，包括外网访问的服务器，基本满足了某人的需要，现在跑在我自己的VPS上，有域名可以直接访问爬完知道豆瓣热门标签下大概有6万多本书，是会不断更新的，所以还要定期爬一下更新一下数据下面是预览，时间所限页面写的糙了点，反正用户就一个－ －


-------------------------answer 5 via  -------------------------


closed


-------------------------answer 6 via  -------------------------


两年前的产物了，不知道还能不能编译。希望可以帮助到有需要的朋友。GitHub - alidawud/mmubee-appGitHub - alidawud/mmubee-server------------------------------------------谢谢大家的关注，由于创业原因（马来西亚版支付宝），很遗憾我无法再继续维护MMUbee，被迫无奈也只好暂停运营。之前有些同学问我分享源代码，当时我也没整理就直接发了。现在我打算等我有空闲的时候整理一下，然后开源到Github。如果有朋友愿意帮我整理请私信我。谢谢。原答案：把学校所有重要的在线服务用爬虫集合成了一套JSON API ，然后开发成了App，并且加了一个类似微信朋友圈的功能，可以说是校友圈吧。全校同学和同班同学都可以通过它互相交流，和微信不同的是，同班之间的交流会有消息推送。App有iOS版和Android版，可以下载并查看教学文档和习题文件，老师有新的通告也会发推送给同学们，还可以查看成绩，课程表，考试时间，个人财务，校园新闻，出勤率等。目前基本全校都在使用。 一个人开发了两个月。App名字叫MMUBee开发这个App，我一毛钱都没赚，每年还赔进去四百多美元的开发者注册费和VPS租用费。两个月里也基本上起早贪黑的做。没太多原因，就是喜欢做东西。我不经常去上课，不过去的时候都可以看见大家在用MMUbee，有一次一个同学打开MMUbee然后对着我说，你快来下载MMUbee，It's awesome!，我说这是我开发的，他没反应过来，过了5秒钟，一脸兴奋的问我Are u kidding me？类似的事情还有很多，比如在上课的时候老师会说，大家不许把考试答案发在MMUbee里。MMUbee的校友圈里，前两个月90%的Post都是好评。Twitter和Facebook上也都是同学们的一片叫好声，校内论坛更是沸沸扬扬了一段时间。虽然MMUbee本身没有盈利，却给我带来了很多机遇，通过这些机遇做了一个上市公司的项目，赚了一大笔。校友圈：多媒体教学系统，列表里是我这学期拿的课，点进去后是老师的通告和课件下载：老师的通告：课程表，出勤，校历，考试时间表，考试成绩等等。


-------------------------answer 7 via  -------------------------


我们用爬虫爬遍整个网络空间，爬那些主流端口，这背后是各种服务，其中 Web 服务最为复杂。我们把这些服务的响应内容尽可能存储下来，加上大量指纹规则去识别它们都是什么。于是我们得到了「全球网络设备」情况：http://www.zoomeye.org/statistic/device由于 Web 服务的特殊性，我们还得到了「全球 Web 服务」情况：http://www.zoomeye.org/statistic/web当我们看到这个时，对整个网络空间充满敬畏，于是内部的项目在 2013 年初考虑对外开放，首先开放了搜索：http://zoomeye.org取了个非常酷的名字：ZoomEye，中文叫：钟馗之眼，定位为：网络空间搜索引擎。当前已经是第三版。由于这种搜索方式非常专业（并非面向普通大众），我们在首页上提供了「用户手册」，还有「搜索 Dork」，用户可以借助这两样快速入门。我们做这个的目的是什么？其实，我们是安全研究者（说通俗点：黑客），我们想解决一个问题：一个漏洞爆发后，我们如何感知全球影响面？这个问题背后的逻辑就是我们做这个搜索引擎的奥秘。在黑客世界，攻击者与防御者，他们对抗的单元都可以细分到一个个「组件」，我们认为组件是构成网络空间的最小单元，比如你搭建一个网站，你要用 Ubuntu/PHP/MySQL/WordPress（附带各种插件）/jQuery 等等等，这些玩意就是一个个组件，你不需要重复劳动去创造它们，而是选择搭积木方式，这是整个网络空间进化的必然结果。正因为如此，如果一个组件出漏洞（这是必然的），那将影响一大批使用它的那些目标。攻击者喜欢这样，因为一个组件出漏洞，攻击者可以大规模黑掉目标，然后做各种坏事（庞大地下产业链的一个关键环节）。对我们来说，其实我们是防御者，我们可以站在攻击者角度去评估这种影响面，然后发出预警。我们最成功的案例是，2014/4/8 心脏出血漏洞爆发时，我们是最快搞定整个权威预警的团队。可以看当时我们基于 ZoomEye 做出的心脏出血全球统计与一年后的相关解读：http://www.zoomeye.org/lab/heartbleedhttp://www.zoomeye.org/lab/heartbleed/2015当时，我们的结论覆盖了央视、新华社、许多科技媒体、很多报刊杂志，还有国家相关监管机构，2014 年底时，入选极客公园评审的 2014 年互联网产品 50 强。这给了我们团队极大的信心，从一个内部实验性小项目逐渐成为安全圈/黑客圈炙手可热的平台。这是一件大工程，如果你仔细体验这个平台，你会认可我的看法。这是一个充满争议的平台，有人说我们通过 ZoomEye 就可以黑遍全世界。我们有能力做这事，但我们不会这样做，黑遍有什么好玩的，好玩在对抗，在促进整个网络空间的良性进化。Google 爬了全球网站，他们说「不作恶」，对我们来说，全球网站只是我们面对庞大网络空间的一个子集（Google 的爬虫复杂度比我们只面对组件的爬虫的复杂度要高 N 个量级，这里不展开），为了把这个平台做好，我们还需要大量的人才与服务器、带宽、钱。这是我这么多年带队做的最酷的爬虫项目，如果你也是 Python 工程师，对黑客这个领域充满好奇，可以私信我^_^


-------------------------answer 8 via  -------------------------


404


-------------------------answer 9 via  -------------------------


用爬虫最大的好处是批量且自动化得获取和处理信息。对于宏观或者微观的情况都可以多一个侧面去了解（不知道能不能帮统计局一些忙）。以下是我们自己用爬虫获得的信息然后做的呈现。（多图预警）1、获得各个机场的实时流量2、获得热点城市的火车票情况3、各种热门公司招聘中的职位数及月薪分布4、某公司的门店变化情况5、对某一类金融产品的检测和跟踪6、对某车型用户数变化情况的跟踪7、对某个App的下载量跟踪
