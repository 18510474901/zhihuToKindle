


如何使用 Python 抓取雪球网页？ - 爬虫（计算机网络） - 知乎






--------------------Link http://www.zhihu.com/question/29311492 ----------------------





--------------------Detail----------------------

我想使用beautifulsoup或者其他的python包  抓取  雪球网页上面的一些组合，因为雪球网的组合持仓变动的时候，雪球网不会给提示，比如说，我想抓取这个http://xueqiu.com/P/ZH010389。基本的想法是用程序追踪到他的持仓，然后有变化的时候，程序给我一个提示。##简而言之，要做的事情是：打开这个界面，然后打开这个界面的调仓历史记录，然后记录下他的当前仓位，对比以前的仓位。##问题是：由于我对HTML不太了解，我打开Chrome的开发者工具的时候，不知道应该怎么样让我的程序打开他的调仓历史记录。。。这个问题可能比较小白。。。麻烦啦！！！

-------------------------answer 0 via  -------------------------


//好多人说现在关注就有提醒的……呃，题主提问的时候显然没那个功能。我写这个只是自己在学习爬虫过程中的练习。我不炒股也不上雪球……//好多赞。容我安利一篇自己的回答如何入门 Python 爬虫？ - 段晓晨的回答边做边调边写~#start coding首先要知道自己在爬什么~楼主说找到HTML的代码云云，思路其实是错误的。因为我们想要的内容不在原始的html里面。但是肯定在浏览器和服务器之间的通信里，我们只要找到这部分数据就好。#我用的是Firefox的FireBug选择网络（Chrome中应该是Network），点击调仓历史记录，如图可以看到浏览器和服务器之间进行了一次通信。我们截获了一个网址。打开看看。可以看到浏览器和服务器之间进行了一次通信。我们截获了一个网址。打开看看。http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol=ZH010389&count=20&page=1看上去像是一堆乱码，但是细心的话就会发现……也就是说我们要的数据都在这里了，所以只要先获取这个页面的内容然后在提取数据就好了~#python3项目，python2中请使用urllib和urllib2
import urllib.request
url = 'http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol=ZH010389&count=20&page=1'
req = urllib.request.Request(url,headers=headers)
html = urllib.request.urlopen(req).read().decode('utf-8')
print(html)
运行一下~报错了~报错没关系，兵来将挡水来土掩~403禁止访问…应该是headers的问题…什么是headers呢…403禁止访问…应该是headers的问题…什么是headers呢…你现在用python去访问网页，网页得到的请求就是你是python程序，但是网页并不想让程序看到自己，因为他是给人看的，资源都被程序占了算什么，所以我们要让python伪装成浏览器。依然是用Firebug查看headers信息。然后我们完善代码在访问过程中添加headers~然后我们完善代码在访问过程中添加headers~import urllib.request
headers = {'X-Requested-With': 'XMLHttpRequest',
           'Referer': 'http://xueqiu.com/p/ZH010389',
           'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0',
           'Host': 'xueqiu.com',
           #'Connection':'keep-alive',
           #'Accept':'*/*',
           'cookie':'s=iabht2os.1dgjn9z; xq_a_token=02a16c8dd2d87980d1b3ddced673bd6a74288bde; xq_r_token=024b1e233fea42dd2e0a74832bde2c914ed30e79; __utma=1.2130135756.1433017807.1433017807.1433017807.1;'
           '__utmc=1; __utmz=1.1433017807.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); Hm_lvt_1db88642e346389874251b5a1eded6e3=1433017809; Hm_lpvt_1db88642e346389874251b5a1eded6e3=1433017809'}

url = 'http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol=ZH010389&count=20&page=1'
req = urllib.request.Request(url,headers=headers)
html = urllib.request.urlopen(req).read().decode('utf-8')
print(html)
这次得到想要的结果了~我们回过头再去看headers会发现，其实有些我并没有写进去，你也可以自己尝试把headers中的某一行注释掉运行。但是每个站是不一样的，你把所有的都填上去是一定能运行成功的，但是可能其中某一些不是必需的。比如我们这里只要有User-Agent（缺少报错403）和cookie（缺少报错400）。好~我们现在拿到了想要的数据，但是看上去太复杂了，一点都不友好。现在我们来解析一下这个网页。其实这个网页是json格式的数据包。然后我们来观察这个数据的解析。然后我们来观察这个数据的解析。#你可以直接点击Firebug中的JSON来看，也可以复制到Notepad++中使用json viewer插件查看。大概是这个样子的……大概是这个样子的……有了json的构成结构我们就可以来解析它了…我直接拿Python Shell调试，一会儿完善代码没什么问题~一切看起来很完美的样子~这一步其实没什么难度，只要你能看懂上一步里我们分析的json数据的组成结构，然后一层一层地向下解析数据就可以了。完善代码。import urllib.request
import json
headers = {#'X-Requested-With': 'XMLHttpRequest',
           #'Referer': 'http://xueqiu.com/p/ZH010389',
           'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0',
           #'Host': 'xueqiu.com',
           #'Connection':'keep-alive',
           #'Accept':'*/*',
           'cookie':'s=iabht2os.1dgjn9z; xq_a_token=02a16c8dd2d87980d1b3ddced673bd6a74288bde; xq_r_token=024b1e233fea42dd2e0a74832bde2c914ed30e79; __utma=1.2130135756.1433017807.1433017807.1433017807.1;'
           '__utmc=1; __utmz=1.1433017807.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); Hm_lvt_1db88642e346389874251b5a1eded6e3=1433017809; Hm_lpvt_1db88642e346389874251b5a1eded6e3=1433017809'}
url = 'http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol=ZH010389&count=20&page=1'
req = urllib.request.Request(url,headers=headers)
html = urllib.request.urlopen(req).read().decode('utf-8')
#print(html)
data = json.loads(html)
print('股票名称',end=':')
print(data['list'][0]['rebalancing_histories'][0]['stock_name'],end='   持仓变化')
print(data['list'][0]['rebalancing_histories'][0]['prev_weight'],end='-->')
print(data['list'][0]['rebalancing_histories'][0]['target_weight'])
print('股票名称',end=':')
print(data['list'][0]['rebalancing_histories'][1]['stock_name'],end='   持仓变化')
print(data['list'][0]['rebalancing_histories'][1]['prev_weight'],end='-->')
print(data['list'][0]['rebalancing_histories'][1]['target_weight'])
 运行程序~好嘞！搞定收工！当然也还不能收工……只是我不干了而已……To-dos:可以看到程序是面向过程的…重复代码很多，可以通过定义类或方法实现调用大概……大概得写点注释……不过这么简单直接无脑面向过程的代码真的需要注释吗如果是想在他持仓变化时收到提醒，需要爬虫定时爬取页面数据与之前数据进行比较如果你更细心的话会发现最初的json网址的构成是这样的…http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol='#此处可添加任意组合的号码例如ZH010389'&count=‘#此处数字是一次获取的交易变化数量,也就是说你一次性拿到了20次的交易,你点开之前交易记录的时候并不会重新请求数据而是读取了本地现有的数据此处数据可以任意修改哦~很神奇的试一试吧~20’&page=‘和前面联系起来,前面是一次性获取20条记录,这边便是页码,通过对page数的控制利用循环可以输出所有交易过程,当然,40一页和20两页的效果显然是一样的,看你怎么玩儿了~1’如果你有耐心看完上面那一大段话的话想必你可以有更多的想法。让别人来指导我们的思路是好的，可是投资的机会稍瞬即逝，跟在别人后面是没有前途的，我们要学习。大数据的时代为什么不试试爬更多人的更多投资记录呢？比如在雪球首页爬取首页推荐的组合，然后自动爬取这些组合所做的所有操作~这样你是不是就有了很厚的一本交易目录，结合过去的股市数据（这些能不能想办法自动获取呢？），你可以自己尝试分析别人作出投资决定的原因（是不是可以把数据自动写入一个excel？提示：xlwt3）…最终指导自己的投资。大数据学习，想想都炫酷。可惜我不炒股…大概就酱紫~希望有帮助~写这么多是因为我自己在学爬虫…一周了…看到实践的机会就来试一下…所以是边调BUG边写答案~大概就写这么多吧…后面的To-dos哪天我突然感兴趣了会试着写一下或者过来补充的…看到这个答案的…前辈还希望多多指教；看到这个答案的新手…欢迎交流：P题主加油！写好了记得贴代码我还能给我老爸用！


-------------------------answer 1 via  -------------------------


现在关注一个组合，就会有持仓变动的提示了。不过我觉得这事情挺有意思的。比如可以把很多持仓的数据都抓下来，做一些综合的分析，看看现在网站上被持有最多的股票是哪一支，某一天被调入最多的又是哪一支之类。于是我决定来抓抓看，顺便借此说说我通常用程序做自动抓取的过程。Step.1 分析页面要抓一个网页，首先自然是要“研究”这个网页。通常我会用两种方式：一个是 Chrome 的 Developer Tools。通过它里面的 Network 功能可以看到页面发出的所有网络请求，而大多数数据请求都会在 XHR 标签下。点击某一个请求，可以看到其具体信息，以及服务器的返回结果。很多网站在对于某些数据会有专门的请求接口，返回一组 json 或者 XML 格式的数据，供前台处理后显示。另一个就是直接查看网页源代码。通常浏览器的右键菜单里都有这个功能。从页面的 HTML 源码里直接寻找你要的数据，分析它格式，为抓取做准备。对于雪球上的一个组合页面 http://xueqiu.com/P/ZH010389，粗略地看了一下它发出的请求，并没有如预想那样直接找到某个数据接口。看源代码，发现有这样一段：SNB.cubeInfo = {"id":10289,"name":"誓把老刀挑下位","symbol":"ZH010389" ...此处略过三千字... "created_date":"2014.11.25"}
SNB.cubePieData = [{"name":"汽车","weight":100,"color":"#537299"}];
cubeInfo 是一个 json 格式的数据，看上去就是我们需要的内容。一般我会找个格式化 json 的网站把数据复制进去方便查看。这应该就是组合的持仓数据。那么接下来，一切似乎都简单了。只要直接发送网页请求，然后把其中 cubeInfo 这段文字取出，按 json 读出数据，就完成了抓取。甚至不用动用什么 BeautifulSoup、正则表达式。Step.2 获取页面分析完毕，开抓。直接 urllib.urlopen 向目标网页发送请求，读出网页。结果，失败了……看了下返回结果：403 Forbidden
You don't have permission to access the URL on this server. Sorry for the inconvenience.
被拒了，所以这种赤裸裸地请求是不行的。没关系，那就稍微包装一下：send_headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36',
    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Connection':'keep-alive',
    'Host':'xueqiu.com',
    'Cookie':r'xxxxxx',
}
req = urllib2.Request(url, headers=send_headers)
resp = urllib2.urlopen(req)
html = resp.read()
header 数据都可以从 Developer Tools 里拿到。这次顺利抓到页面内容。一般网站或多或少都会对请求来源做一些阻拦，通过加 header 可以搞定大部分情况。Step.3 提取数据因为这个数据比较明显，直接用通过一些字符串查找和截取操作就可以取出来。pos_start = html.find('SNB.cubeInfo = ') + len('SNB.cubeInfo = ')
pos_end = html.find('SNB.cubePieData')
data = html[pos_start:pos_end]
dic = json.loads(data)
dic 就是一个包含数据的字典对象。之后想干什么就随便你了。对于复杂一点的情况，可以通过 BeautifulSoup 来定位 html 标签。再不好办的，就用正则表达式，基本都可以解决掉。Step.4 处理数据因为我想对数据进行持久化存储，并且做展示和分析，所以我用了 django 里的 ORM 来处理抓下来的数据。# add Portfolio
portfolio, c = models.Portfolio.objects.get_or_create(code=dic['symbol'])
portfolio.name = dic['name']
portfolio.earnings = dic['total_gain']
portfolio.save()
# add Stock
stocks = dic['view_rebalancing']['holdings']
for s in stocks:
    stock, c = models.Stock.objects.get_or_create(code=s['stock_symbol'])
    stock.name = s['stock_name']
    stock.count += 1
    stock.weight += s['weight']
    stock.save()
Portfolio 记录下组合及其收益，Stock则记录每支股票的被收录数和总收录份额。对于抓取到的，一般也可以存在文件中，或者直接通过 SQL 存入数据库，视不同情况和个人喜好而定。Step.5 批量抓取前面的一套做下来，就完整地抓取了一组数据。要达到目的，还要设计一下批量抓取的程序。一个要解决的问题就是如何获得组合列表。这个可以再通过另一个抓取程序来实现。然后根据这些列表来循环抓取就可以了。若要细究，还要考虑列表如何保存和使用，如何处理抓取失败和重复抓取，如何控制抓取频率防止被封，可否并行抓取等等。Step.6 数据分析数据有了，你要怎么用它，这是个很大的问题。可以简单的统计现象，也可以想办法深入分析背后隐藏的逻辑。不多说，我也还只是在摸索之中。抓取的代码也放在了我的 Github 上：crossin/avalanche · GitHub


-------------------------answer 2 via  -------------------------


需要两个模块配合:爬虫模块：单纯负责抓取和存储数据数据处理模块：处理爬虫存储的数据。如发现某个人某个持仓数据发生了变化，向你发出通知该爬虫的简单的流程:定时访问目标页面抓取当前目标页面的数据，存入数据库数据处理模块简单的流程：定时访问数据库数据库中的数据满足某个条件时执行自己设定的动作


-------------------------answer 3 via  -------------------------


雪球网已经改了很多规则，以前的很多代码估计都不能用了我刚写了一个雪球网的模拟登录，fuck-login/012 xueqiu.com at master · xchaoinfo/fuck-login · GitHub在此基础上修改，可以达到题主的目的，而且可以做到更加简单。处理 cookies ，不需要每次都登录一次的方法，可以参考 fuck-login/001 zhihu at master · xchaoinfo/fuck-login · GitHub 的处理方法。


-------------------------answer 4 via  -------------------------


抓取雪球的数据？巧了，刚看到一篇文章专门讲这个的，推荐给大家：互联网金融爬虫怎么写


-------------------------answer 5 via  -------------------------


已关注的组合会收到调仓通知。#技术宅都好暴力，看不到调仓就直接抓......#


-------------------------answer 6 via  -------------------------


我在 @段晓晨的基础上做了一点点优化，目前是这样的。测试前请把帐号密码填上更新内容：增加了自动获取cookie修改了一下显示组合改变的代码import urllib.request
import json
import http.cookiejar

#设置cookie
CookieJar = http.cookiejar.CookieJar()
CookieProcessor = urllib.request.HTTPCookieProcessor(CookieJar)
opener = urllib.request.build_opener(CookieProcessor)
urllib.request.install_opener(opener)

#登陆获得cookie
params = urllib.parse.urlencode({'username':'*****','password':'*****'}).encode(encoding='UTF8')
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0'}
request = urllib.request.Request('http://xueqiu.com/user/login',headers=headers)
httpf = opener.open(request, params)

#获得组合
url = 'http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol=ZH340739&count=20&page=1'
req = urllib.request.Request(url,headers=headers)
html = urllib.request.urlopen(req).read().decode('utf-8')
data = json.loads(html)
stockdata = data['list'][0]['rebalancing_histories']
for i in range(len(stockdata)):
    print('股票名称',end=':')
    print(stockdata[i]['stock_name'],end='   持仓变化')
    print(stockdata[i]['prev_weight'],end='-->')
    print(stockdata[i]['target_weight'])



-------------------------answer 7 via  -------------------------


首先需要三个库：urllib2，cookielib，json然后用firefox 打开誓把老刀挑下位 并登陆，然后找到 cookie文件，最后调仓记录的地址是：http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol=ZH010389&count=20&page=1   用urllib2 和coolielib 伪造header，和cookie 访问 就可以得到 json文件格式的调仓记录，然后用json 处理 就可以了 


-------------------------answer 8 via  -------------------------


题主不知道关注后有推送提示么 ......


-------------------------answer 9 via  -------------------------


cookie应该会有失效时间吧，是不是还是要模拟登录来获得最新cookie？ 
